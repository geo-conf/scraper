@inproceedings{10.1145/3379337.3422374,
author = {Bernstein, Michael S. and Greif, Irene and Mackay, Wendy and Ishii, Hiroshi and Grudin, Jonathan and Karahalios, Karrie and Ringel Morris, Meredith and Kittur, Aniket and Teevan, Jaime and Zhang, Amy X. and Salehi, Niloufar},
title = {UIST+CSCW: A Celebration of Systems Research in Collaborative and Social Computing},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3422374},
doi = {10.1145/3379337.3422374},
abstract = {This joint panel between UIST and CSCW brings together leading researchers at the intersection of the conferences-systems researchers in collaborative and social computing-to engage in a discussion and retrospective. Pairs of panelists will represent each decade since the founding of the conferences, sharing a brief retrospective that surveys the most influential papers of that decade, the zeitgeist of the problems that were popular that decade and why, and what each decade's work has to say to the decades that came before and after. The panel is intended as a space to celebrate advances in the field, and reflect on the burdens and opportunities that it faces ahead.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1–3},
numpages = {3},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3422878,
author = {Smith, David A.},
title = {The Augmented Conversation and the Amplified World},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3422878},
doi = {10.1145/3379337.3422878},
abstract = {Human communication mediated by computers and Augmented Reality devices will enable us to dynamically express, share and explore new ideas with each other via live simulations as easily as we talk about the weather. This collaboration provides a "shared truth" - what you see is exactly what I see, I see you perform an action as you do it, and we both see exactly the same dynamic transformation of this shared information space. When you express an idea, the computer, a full participant in this conversation, instantly makes it real for both of us enabling us to critique and negotiate the meaning of it. This shared virtual world will be as live, dynamic, pervasive, and visceral as the physical.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {4},
numpages = {1},
keywords = {collaboration, ar, vr, shared truth, replicated computation, croquet},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3422877,
author = {Hodges, Steve},
title = {Democratizing the Production of Interactive Hardware},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3422877},
doi = {10.1145/3379337.3422877},
abstract = {The development of new hardware can be split into two phases: prototyping and production. A wide variety of tools and techniques have empowered people to build prototypes during the first phase, but the transition to production is still complex, costly and prone to failure. This means the second phase often requires an up-front commitment to large volume production in order to be viable. I believe that new tools and techniques can democratize hardware production. Imagine "DevOps for hardware" - everything from circuit simulation tools to re-usable hardware test jig designs; and from test-driven development for hardware to telepresence for remote factory visits. Supporting low volume production and organic scaling in this way would spur innovation and increase consumer choice. I encourage the UIST community to join me in pursuit of this vision.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {5–6},
numpages = {2},
keywords = {democratizing hardware, design verification, low volume electronics, long tail hardware, manufacturing and test},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415874,
author = {Brudy, Frederik and Ledo, David and Pahud, Michel and Henry Riche, Nathalie and Holz, Christian and Waghmare, Anand and Surale, Hemant Bhaskar and Peinado, Marcus and Zhang, Xiaokuan and Joyner, Shannon and Chandramouli, Badrish and Minhas, Umar Farooq and Goldstein, Jonathan and Buxton, William and Hinckley, Ken},
title = {SurfaceFleet: Exploring Distributed Interactions Unbounded from Device, Application, User, and Time},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415874},
doi = {10.1145/3379337.3415874},
abstract = {Knowledge work increasingly spans multiple computing surfaces. Yet in status quo user experiences, content as well as tools, behaviors, and workflows are largely bound to the current device-running the current application, for the current user, and at the current moment in time. SurfaceFleet is a system and toolkit that uses resilient distributed programming techniques to explore cross-device interactions that are unbounded in these four dimensions of device, application, user, and time. As a reference implementation, we describe an interface built using SurfaceFleet that employs lightweight, semi-transparent UI elements known as Applets. Applets appear always-on-top of the operating system, application windows, and (conceptually) above the device itself. But all connections and synchronized data are virtualized and made resilient through the cloud. For example, a sharing Applet known as a Portfolio allows a user to drag and drop unbound Interaction Promises into a document. Such promises can then be fulfilled with content asynchronously, at a later time (or multiple times), from another device, and by the same or a different user.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {7–21},
numpages = {15},
keywords = {distributed systems, mobility, cross-device interaction},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415830,
author = {Gori, Julien and Han, Han L. and Beaudouin-Lafon, Michel},
title = {FileWeaver: Flexible File Management with Automatic Dependency Tracking},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415830},
doi = {10.1145/3379337.3415830},
abstract = {Knowledge management and sharing involves a variety of specialized but isolated software tools, tied together by the files that these tools use and produce. We interviewed 23 scientists and found that they all had difficulties using the file system to keep track of, re-find and maintain consistency among related but distributed information. We introduce FileWeaver, a system that automatically detects dependencies among files without explicit user action, tracks their history, and lets users interact directly with the graphs representing these dependencies and version history. Changes to a file can trigger recipes, either automatically or under user control, to keep the file consistent with its dependants. Users can merge variants of a file, e.g. different output formats, into a polymorphic file, or morph, and automate the management of these variants. By making dependencies among files explicit and visible, FileWeaver facilitates the automation of workflows by scientists and other users who rely on the file system to manage their data.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {22–34},
numpages = {13},
keywords = {file system, version management, workflows, dependency management},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415841,
author = {Yang, Jackie (Junrui) and Lam, Monica S. and Landay, James A.},
title = {DoThisHere: Multimodal Interaction to Improve Cross-Application Tasks on Mobile Devices},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415841},
doi = {10.1145/3379337.3415841},
abstract = {Many computing tasks, such as comparison shopping, two-factor authentication, and checking movie reviews, require using multiple apps together. On large screens, "windows, icons, menus, pointer" (WIMP) graphical user interfaces (GUIs) support easy sharing of content and context between multiple apps. So, it is straightforward to see the content from one application and write something relevant in another application, such as looking at the map around a place and typing walking instructions into an email. However, although today's smartphones also use GUIs, they have small screens and limited windowing support, making it hard to switch contexts and exchange data between apps.  We introduce DoThisHere, a multimodal interaction technique that streamlines cross-app tasks and reduces the burden these tasks impose on users. Users can use voice to refer to information or app features that are off-screen and touch to specify where the relevant information should be inserted or is displayed. With DoThisHere, users can access information from or carry information to other apps with less context switching.  We conducted a survey to find out what cross-app tasks people are currently performing or wish to perform on their smartphones. Among the 125 tasks that we collected from 75 participants, we found that 59 of these tasks are not well supported currently. DoThisHere is helpful in completing 95% of these unsupported tasks. A user study, where users are shown the list of supported voice commands when performing a representative sample of such tasks, suggests that DoThisHere may reduce expert users' cognitive load; the Query action, in particular, can help users reduce task completion time.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {35–44},
numpages = {10},
keywords = {voice interfaces, cross-app tasks, multimodal interaction},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415855,
author = {Luo, Danli and Gu, Jianzhe and Qin, Fang and Wang, Guanyun and Yao, Lining},
title = {E-Seed: Shape-Changing Interfaces That Self Drill},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415855},
doi = {10.1145/3379337.3415855},
abstract = {As sensors and interactive devices become ubiquitous and transition outdoors and into the wild, we are met with the challenge of mass deployment and actuation. We present E-seed, a biomimetic platform that consumes little power to deploy, harvests energy from nature to install, and functions autonomously in the field. Each seed can individually self-drill into a substrate by harvesting moisture fluctuations in its ambient environment. As such, E-seed acts as a shape-changing interface to autonomously embed functional devices and interfaces into the soil, with the potential of aerial deployment in hard-to-reach locations. Our system is constructed primarily from wood veneer, making it lightweight, inexpensive, and biodegradable. In this paper, we detail our fabrication process and showcase demos that leverage the E-seed platform as a self-drilling interface. We envision that possible applications include soil sensors, sampling, and environmental monitoring for agriculture and reforestation.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {45–57},
numpages = {13},
keywords = {bio-inspired design, smart materials, human-nature interaction, biohybrid interface, morphing materials, internet of things, shape-changing interface, biomimetic design},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415853,
author = {Noma, Yuta and Narumi, Koya and Okuya, Fuminori and Kawahara, Yoshihiro},
title = {Pop-up Print: Rapidly 3D Printing Mechanically Reversible Objects in the Folded State},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415853},
doi = {10.1145/3379337.3415853},
abstract = {Despite recent advancements in 3D printing technology, which allows users to rapidly produce 3D objects, printing tall and/or large objects still consumes more time and large amount of support material. In order to address these problems, we propose Pop-up Print, a method to 3D print an object in a compact "folded" state and then unfold it after printing to achieve the final artifact. Using this method, we can reduce the object's print height and volume, which directly affects the printing time and support material consumption. In addition, thanks to the reversibility of folding/unfolding, we can reversibly minimize the printed object's volume when unused for storage or transportation, and expand it only in use. To achieve Pop-up Print, we first conducted an experiment using selected printed sample objects with several parameters, in order to determine suitable crease patterns that make both the unfolded and folded state mechanically stable. Based on this result, we developed an interactive design tool to convert 3D models - such as a Stanford Bunny or a Huffman's cone - to the folded shape. Our design tool allows users to decide non-intuitive parameters that may affect the form's mechanical stability, while maintaining both functional crease patterns and the object's original form factor. Finally, we demonstrate the feasibility of our method through several examples of folded objects.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {58–70},
numpages = {13},
keywords = {digital fabrication, origami structure, 3d printing, cad, interactive design tool},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415867,
author = {Yang, Humphrey and Qian, Kuanren and Liu, Haolin and Yu, Yuxuan and Gu, Jianzhe and McGehee, Matthew and Zhang, Yongjie Jessica and Yao, Lining},
title = {SimuLearn: Fast and Accurate Simulator to Support Morphing Materials Design and Workflows},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415867},
doi = {10.1145/3379337.3415867},
abstract = {Morphing materials allow us to create new modalities of interaction and fabrication by leveraging the materials? dynamic behaviors. Yet, despite the ongoing rapid growth of computational tools within this realm, current developments are bottlenecked by the lack of an effective simulation method. As a result, existing design tools must trade-off between speed and accuracy to support a real-time interactive design scenario. In response, we introduce SimuLearn, a data-driven method that combines finite element analysis and machine learning to create real-time (0.61 seconds) and truthful (97% accuracy) morphing material simulators. We use mesh-like 4D printed structures to contextualize this method and prototype design tools to exemplify the design workflows and spaces enabled by a fast and accurate simulation method. Situating this work among existing literature, we believe SimuLearn is a timely addition to the HCI CAD toolbox that can enable the proliferation of morphing materials.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {71–84},
numpages = {14},
keywords = {4d printing, computational fabrication., shape-changing interface, simulation, design tool, machine learning},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415889,
author = {Kim, Daehwa and Park, Keunwoo and Lee, Geehyuk},
title = {OddEyeCam: A Sensing Technique for Body-Centric Peephole Interaction Using WFoV RGB and NFoV Depth Cameras},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415889},
doi = {10.1145/3379337.3415889},
abstract = {The space around the body not only expands the interaction space of a mobile device beyond its small screen, but also enables users to utilize their kinesthetic sense. Therefore, body-centric peephole interaction has gained considerable attention. To support its practical implementation, we propose OddEyeCam, which is a vision-based method that tracks the 3D location of a mobile device in an absolute, wide, and continuous manner with respect to the body of a user in both static and mobile environments. OddEyeCam tracks the body of a user using a wide-view RGB camera and obtains precise depth information using a narrow-view depth camera from a smartphone close to the body. We quantitatively evaluated OddEyeCam through an accuracy test and two user studies. The accuracy test showed the average tracking accuracy of OddEyeCam was 4.17 and 4.47cm in 3D space when a participant is standing and walking, respectively. In the frst user study, we implemented various interaction scenarios and observed that OddEyeCam was well received by the participants. In the second user study, we observed that the peephole target acquisition task performed using our system followed Fitts? law. We also analyzed the performance of OddEyeCam using the obtained measurements and observed that the participants completed the tasks with suffcient speed and accuracy.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {85–97},
numpages = {13},
keywords = {mobile device, peephole display, spatially-aware display, 3d, sensing, body-centric interaction},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415856,
author = {Hwang, Dong-Hyun and Aso, Kohei and Yuan, Ye and Kitani, Kris and Koike, Hideki},
title = {MonoEye: Multimodal Human Motion Capture System Using A Single Ultra-Wide Fisheye Camera},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415856},
doi = {10.1145/3379337.3415856},
abstract = {We present MonoEye, a multimodal human motion capture system using a single RGB camera with an ultra-wide fisheye lens, mounted on the user's chest. Existing optical motion capture systems use multiple cameras, which are synchronized and require camera calibration. These systems also have usability constraints that limit the user's movement and operating space. Since the MonoEye system is based on a wearable single RGB camera, the wearer's 3D body pose can be captured without space and environment limitations. The body pose, captured with our system, is aware of the camera orientation and therefore it is possible to recognize various motions that existing egocentric motion capture systems cannot recognize. Furthermore, the proposed system captures not only the wearer's body motion but also their viewport using the head pose estimation and an ultra-wide image. To implement robust multimodal motion capture, we design three deep neural networks: BodyPoseNet, HeadPoseNet, and CameraPoseNet, that estimate 3D body pose, head pose, and camera pose in real-time, respectively. We train these networks with our new extensive synthetic dataset providing 680K frames of renderings of people with a wide range of body shapes, clothing, actions, backgrounds, and lighting conditions. To demonstrate the interactive potential of the MonoEye system, we present several application examples from common body gestural to context-aware interactions.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {98–111},
numpages = {14},
keywords = {egocentric camera, head pose estimation, fisheye camera, gaze direction, computer vision, mobile motion capture},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415879,
author = {Chen, Tuochao and Steeper, Benjamin and Alsheikh, Kinan and Tao, Songyun and Guimbreti\`{e}re, Fran\c{c}ois and Zhang, Cheng},
title = {C-Face: Continuously Reconstructing Facial Expressions by Deep Learning Contours of the Face with Ear-Mounted Miniature Cameras},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415879},
doi = {10.1145/3379337.3415879},
abstract = {C-Face (Contour-Face) is an ear-mounted wearable sensing technology that uses two miniature cameras to continuously reconstruct facial expressions by deep learning contours of the face. When facial muscles move, the contours of the face change from the point of view of the ear-mounted cameras. These subtle changes are fed into a deep learning model which continuously outputs 42 facial feature points representing the shapes and positions of the mouth, eyes and eyebrows. To evaluate C-Face, we embedded our technology into headphones and earphones. We conducted a user study with nine participants. In this study, we compared the output of our system to the feature points outputted by a state of the art computer vision library (Dlib) from a font facing camera. We found that the mean error of all 42 feature points was 0.77 mm for earphones and 0.74 mm for headphones. The mean error for 20 major feature points capturing the most active areas of the face was 1.43 mm for earphones and 1.39 mm for headphones. The ability to continuously reconstruct facial expressions introduces new opportunities in a variety of applications. As a demonstration, we implemented and evaluated C-Face for two applications: facial expression detection (outputting emojis) and silent speech recognition. We further discuss the opportunities and challenges of deploying C-Face in real-world applications.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {112–125},
numpages = {14},
keywords = {ear sensing, emoji recognition, wearable computing, silent speech, computer vision: facial expression reconstruction and tracking, deep learning},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415890,
author = {Yu, Geoffrey X. and Grossman, Tovi and Pekhimenko, Gennady},
title = {Skyline: Interactive In-Editor Computational Performance Profiling for Deep Neural Network Training},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415890},
doi = {10.1145/3379337.3415890},
abstract = {Training a state-of-the-art deep neural network (DNNs) is a computationally-expensive and time-consuming process, which incentivizes deep learning developers to debug their DNNs for computational performance. However, effectively performing this debugging requires intimate knowledge about the underlying software and hardware systems-something that the typical deep learning developer may not have. To help bridge this gap, we present Skyline: a new interactive tool for DNN training that supports in-editor computational performance profiling, visualization, and debugging. Skyline's key contribution is that it leverages special computational properties of DNN training to provide (i) interactive performance predictions and visualizations, and (ii) directly manipulatable visualizations that, when dragged, mutate the batch size in the code. As an in-editor tool, Skyline allows users to leverage these diagnostic features to debug the performance of their DNNs during development. An exploratory qualitative user study of Skyline produced promising results; all the participants found Skyline to be useful and easy to use.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {126–139},
numpages = {14},
keywords = {visualization, debugging, deep neural networks, interactive performance profiling, machine learning, skyline},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415842,
author = {Kery, Mary Beth and Ren, Donghao and Hohman, Fred and Moritz, Dominik and Wongsuphasawat, Kanit and Patel, Kayur},
title = {Mage: Fluid Moves Between Code and Graphical Work in Computational Notebooks},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415842},
doi = {10.1145/3379337.3415842},
abstract = {We aim to increase the flexibility at which a data worker can choose the right tool for the job, regardless of whether the tool is a code library or an interactive graphical user interface (GUI). To achieve this flexibility, we extend computational notebooks with a new API mage, which supports tools that can represent themselves as both code and GUI as needed. We discuss the design of mage as well as design opportunities in the space of flexible code/GUI tools for data work. To understand tooling needs, we conduct a study with nine professional practitioners and elicit their feedback on mage and potential areas for flexible code/GUI tooling. We then implement six client tools for mage that illustrate the main themes of our study findings. Finally, we discuss open challenges in providing flexible code/GUI interactions for data workers.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {140–151},
numpages = {12},
keywords = {data science programming, handoff, machine learning programming, computational notebooks},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415851,
author = {Wu, Yifan and Hellerstein, Joseph M. and Satyanarayan, Arvind},
title = {B2: Bridging Code and Interactive Visualization in Computational Notebooks},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415851},
doi = {10.1145/3379337.3415851},
abstract = {Data scientists have embraced computational notebooks to author analysis code and accompanying visualizations within a single document. Currently, although these media may be interleaved, they remain siloed: interactive visualizations must be manually specified as they are divorced from the analysis provenance expressed via dataframes, while code cells have no access to users' interactions with visualizations, and hence no way to operate on the results of interaction. To bridge this divide, we present B2, a set of techniques grounded in treating data queries as a shared representation between the code and interactive visualizations. B2 instruments data frames to track the queries expressed in code and synthesize corresponding visualizations. These visualizations are displayed in a dashboard to facilitate interactive analysis. When an interaction occurs, B2 reifies it as a data query and generates a history log in a new code cell. Subsequent cells can use this log to further analyze interaction results and, when marked as reactive, to ensure that code is automatically recomputed when new interaction occurs. In an evaluative study with data scientists, we find that B2 promotes a tighter feedback loop between coding and interacting with visualizations. All participants frequently moved from code to visualization and vice-versa, which facilitated their exploratory data analysis in the notebook.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {152–165},
numpages = {14},
keywords = {computational notebooks, exploratory programming, data science, interactive visualizations},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415892,
author = {Suzuki, Ryo and Kazi, Rubaiat Habib and Wei, Li-yi and DiVerdi, Stephen and Li, Wilmot and Leithinger, Daniel},
title = {RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415892},
doi = {10.1145/3379337.3415892},
abstract = {We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid-air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {166–181},
numpages = {16},
keywords = {embedded data visualization, real-time authoring, sketching interfaces, tangible interaction, augmented reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415827,
author = {Thoravi Kumaravel, Balasaravanan and Nguyen, Cuong and DiVerdi, Stephen and Hartmann, Bjoern},
title = {TransceiVR: Bridging Asymmetrical Communication Between VR Users and External Collaborators},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415827},
doi = {10.1145/3379337.3415827},
abstract = {Virtual Reality (VR) users often need to work with other users, who observe them outside of VR using an external display. Communication between them is difficult; the VR user cannot see the external user's gestures, and the external user cannot see VR scene elements outside of the VR user's view. We carried out formative interviews with experts to understand these asymmetrical interactions and identify their goals and challenges. From this, we identify high-level system design goals to facilitate asymmetrical interactions and a corresponding space of implementation approaches based on the level of programmatic access to a VR application. We present TransceiVR, a system that utilizes VR platform APIs to enable asymmetric communication interfaces for third-party applications without requiring source code access. TransceiVR allows external users to explore the VR scene spatially or temporally, to annotate elements in the VR scene at correct depths, and to discuss via a shared static virtual display. An initial co-located user evaluation with 10 pairs shows that our system makes asymmetric collaborations in VR more effective and successful in terms of task time, error rate, and task load index. An informal evaluation with a remote expert gives additional insight on utility of features for real world tasks.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {182–195},
numpages = {14},
keywords = {asymmetric interactions, virtual reality, collaboration},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415591,
author = {Clarke, Christopher and Cavdir, Doga and Chiu, Patrick and Denoue, Laurent and Kimber, Don},
title = {Reactive Video: Adaptive Video Playback Based on User Motion for Supporting Physical Activity},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415591},
doi = {10.1145/3379337.3415591},
abstract = {Videos are a convenient platform to begin, maintain, or improve a fitness program or physical activity. Traditional video systems allow users to manipulate videos through specific user interface actions such as button clicks or mouse drags, but have no model of what the user is doing and are unable to adapt in useful ways. We present adaptive video playback, which seamlessly synchronises video playback with the user's movements, building upon the principle of direct manipulation video navigation. We implement adaptive video playback in Reactive Video, a vision-based system which supports users learning or practising a physical skill. The use of pre-existing videos removes the need to create bespoke content or specially authored videos, and the system can provide real-time guidance and feedback to better support users when learning new movements. Adaptive video playback using a discrete Bayes and particle filter are evaluated on a data set collected of participants performing tai chi and radio exercises. Results show that both approaches can accurately adapt to the user's movements, however reversing playback can be problematic.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {196–208},
numpages = {13},
keywords = {physical activity, direct manipulation, probabilistic, full body},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415891,
author = {Bouzbib, Elodie and Bailly, Gilles and Haliyo, Sinan and Frey, Pascal},
title = {CoVR: A Large-Scale Force-Feedback Robotic Interface for Non-Deterministic Scenarios in VR},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415891},
doi = {10.1145/3379337.3415891},
abstract = {We present CoVR, a novel robotic interface providing strong kinesthetic feedback (100 N) in a room-scale VR arena. It consists of a physical column mounted on a 2D Cartesian ceiling robot (XY displacements) with the capacity of (1) resisting to body-scaled users' actions such as pushing or leaning; (2) acting on the users by pulling or transporting them as well as (3) carrying multiple potentially heavy objects (up to 80kg) that users can freely manipulate or make interact with each other. We describe its implementation and define a trajectory generation algorithm based on a novel user intention model to support non-deterministic scenarios, where the users are free to interact with any virtual object of interest with no regards to the scenarios' progress. A technical evaluation and a user study demonstrate the feasibility and usability of CoVR, as well as the relevance of whole-body interactions involving strong forces, such as being pulled through or transported.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {209–222},
numpages = {14},
keywords = {encountered-type haptic devices, virtual reality, tangible user interface, robotic graphics, kinesthetic feedback, haptics, actuated device},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415859,
author = {Yixian, Yan and Takashima, Kazuki and Tang, Anthony and Tanno, Takayuki and Fujita, Kazuyuki and Kitamura, Yoshifumi},
title = {ZoomWalls: Dynamic Walls That Simulate Haptic Infrastructure for Room-Scale VR World},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415859},
doi = {10.1145/3379337.3415859},
abstract = {We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user's movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {223–235},
numpages = {13},
keywords = {immersive experience, encountered-type haptic devices},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415870,
author = {Gonzalez, Eric J. and Abtahi, Parastoo and Follmer, Sean},
title = {REACH+: Extending the Reachability of Encountered-Type Haptics Devices through Dynamic Redirection in VR},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415870},
doi = {10.1145/3379337.3415870},
abstract = {Encountered-type haptic devices (EHDs) face a number of challenges when physically embodying content in a virtual environment, including workspace limits and device latency. To address these issues, we propose REACH+, a framework for dynamic visuo-haptic redirection to improve the perceived performance of EHDs during physical interaction in VR. Using this approach, we estimate the user's arrival time to their intended target and redirect their hand to a point within the EHD's spatio-temporally reachable space. We present an evaluation of this framework implemented with a desktop mobile robot in a 2D target selection task, tested at four robot speeds (20, 25, 30 and 35 cm/s). Results suggest that REACH+ can improve the performance of lower-speed EHDs, increasing their rate of on-time arrival to the point of contact by up to 25% and improving users? self-reported sense of realism.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {236–248},
numpages = {13},
keywords = {visuo-haptic illusion, virtual reality, retargeting, encountered-type haptics, redirection},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415825,
author = {Fosco, Camilo and Casser, Vincent and Bedi, Amish Kumar and O'Donovan, Peter and Hertzmann, Aaron and Bylinskii, Zoya},
title = {Predicting Visual Importance Across Graphic Design Types},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415825},
doi = {10.1145/3379337.3415825},
abstract = {This paper introduces a Unified Model of Saliency and Importance (UMSI), which learns to predict visual importance in input graphic designs, and saliency in natural images, along with a new dataset and applications. Previous methods for predicting saliency or visual importance are trained individually on specialized datasets, making them limited in application and leading to poor generalization on novel image classes, while requiring a user to know which model to apply to which input. UMSI is a deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input. This allows the model to work more effectively without requiring a user to label the input. We also introduce Imp1k, a new dataset of designs annotated with importance information. We demonstrate two new design interfaces that use importance prediction, including a tool for adjusting the relative importance of design elements, and a tool for reflowing designs to new aspect ratios while preserving visual importance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {249–260},
numpages = {12},
keywords = {importance, graphic designs, automated design, user interface for design, human attention, saliency, deep learning},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415866,
author = {Shimizu, Evan and Fisher, Matthew and Paris, Sylvain and McCann, James and Fatahalian, Kayvon},
title = {Design Adjectives: A Framework for Interactive Model-Guided Exploration of Parameterized Design Spaces},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415866},
doi = {10.1145/3379337.3415866},
abstract = {Many digital design tasks require a user to set a large number of parameters. Gallery-based interfaces provide a way to quickly evaluate examples and explore the space of potential designs, but require systems to predict which designs from a high-dimensional space are the right ones to present to the user. In this paper we present the design adjectives framework for building parameterized design tools in high dimensional design spaces. The framework allows users to create and edit design adjectives, machine-learned models of user intent, to guide exploration through high-dimensional design spaces. We provide a domain-agnostic implementation of the design adjectives framework based on Gaussian process regression, which is able to rapidly learn user intent from only a few examples. Learning and sampling of the design adjective occurs at interactive rates, making the system suitable for iterative design workflows. We demonstrate use of the design adjectives framework to create design tools for three domains: materials, fonts, and particle systems. We evaluate these tools in a user study showing that participants were able to easily explore the design space and find designs that they liked, and in professional case studies that demonstrate the framework's ability to support professional design concepting workflows.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {261–278},
numpages = {18},
keywords = {creativity support, design tools, interactive interfaces, sampling methods},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415814,
author = {Chi, Peggy and Sun, Zheng and Panovich, Katrina and Essa, Irfan},
title = {Automatic Video Creation From a Web Page},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415814},
doi = {10.1145/3379337.3415814},
abstract = {Creating marketing videos from scratch can be challenging, especially when designing for multiple platforms with different viewing criteria. We present URL2Video, an automatic approach that converts a web page into a short video given temporal and visual constraints. URL2Video captures quality materials and design styles extracted from a web page, including fonts, colors, and layouts. Using constraint programming, URL2Video's design engine organizes the visual assets into a sequence of shots and renders to a video with user-specified aspect ratio and duration. Creators can review the video composition, modify constraints, and generate video variation through a user interface. We learned the design process from designers and compared our automatically generated results with their creation through interviews and an online survey. The evaluation shows that URL2Video effectively extracted design elements from a web page and supported designers by bootstrapping the video creation process.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {279–292},
numpages = {14},
keywords = {web design, video creation, storytelling, web document, creativity tools, video storyboard},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415895,
author = {Roumen, Thijs and Apel, Ingo and Shigeyama, Jotaro and Muhammad, Abdullah and Baudisch, Patrick},
title = {Kerf-Canceling Mechanisms: Making Laser-Cut Mechanisms Operate across Different Laser Cutters},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415895},
doi = {10.1145/3379337.3415895},
abstract = {Getting laser-cut mechanisms, such as those in micro-scopes, robots, vehicles, etc., to work, requires all their components to be dimensioned precisely. This precision, however, tends to be lost when fabricating on a differ-ent laser cutter, as it is likely to remove more or less mate-rial (aka 'kerf'). We address this with what we call kerf-canceling mechanisms. Kerf-canceling mechanisms replace laser-cut bearings, sliders, gear pairs, etc. Unlike their tradi-tional counterparts, however, they keep working when manufactured on a different laser cutter and/or with different kerf. Kerf-canceling mechanisms achieve this by adding an additional wedge element per mechanism. We have created a software tool KerfCanceler that locates traditional mecha-nisms in cutting plans and replaces them with their kerf-canceling counterparts. We evaluated our tool by converting 17 models found online to kerf-invariant models; we evaluated kerf-canceling bearings by testing with kerf values ranging from 0mm and 0.5mm and find that they perform reliably independent of this kerf.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {293–303},
numpages = {11},
keywords = {sharing, portable fabrication, reuse, personal fabrication},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415885,
author = {Leen, Danny and Peek, Nadya and Ramakers, Raf},
title = {LamiFold: Fabricating Objects with Integrated Mechanisms Using a Laser Cutter Lamination Workflow},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415885},
doi = {10.1145/3379337.3415885},
abstract = {We present LamiFold, a novel design and fabrication workflow for making functional mechanical objects using a laser cutter. Objects fabricated with LamiFold embed advanced rotary, linear, and chained mechanisms, including linkages that support fine-tuning and locking position. Laser cutting such mechanisms without LamiFold requires designing for and embedding off-the-shelf parts such as springs, bolts, and axles for gears. The key to laser cutting our functional mechanisms is the selective cutting and gluing of stacks of sheet material. Designing mechanisms for this workflow is non-trivial, therefore we contribute a set of mechanical primitives that are compatible with our lamination workflow and can be combined to realize advanced mechanical systems. Our software design environment facilitates the process of inserting and composing our mechanical primitives and realizing functional laser-cut objects.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {304–316},
numpages = {13},
keywords = {lamination, mechanical design, personal fabrication, cad},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415899,
author = {Larsson, Maria and Yoshida, Hironori and Umetani, Nobuyuki and Igarashi, Takeo},
title = {Tsugite: Interactive Design and Fabrication of Wood Joints},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415899},
doi = {10.1145/3379337.3415899},
abstract = {We present Tsugite - an interactive system for designing and fabricating wood joints for frame structures. To design and manually craft such joints is difficult and time consuming. Our system facilitates the creation of custom joints by a modeling interface combined with computer numerical control (CNC) fabrication. The design space is a 3D grid of voxels that enables efficient geometrical analysis and combinatorial search. The interface has two modes: manual editing and gallery. In the manual editing mode, the user edits a joint while receiving real-time graphical feedback and suggestions provided based on performance metrics including slidability, fabricability, and durability with regard to the direction of fiber. In the gallery mode, the user views and selects feasible joints that have been pre-calculated. When a joint design is finalized, it can be manufactured with a 3-axis CNC milling machine using a specialized path planning algorithm that ensures joint assemblability by corner rounding. This system was evaluated via a user study and by designing and fabricating joint samples and functional furniture.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {317–327},
numpages = {11},
keywords = {computer-aided manufacturing, human-computer interaction, joinery, path planning, subtractive manufacturing, computer numerical control, computer-aided design},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415815,
author = {Wang, Tianyi and Qian, Xun and He, Fengming and Hu, Xiyun and Huo, Ke and Cao, Yuanzhi and Ramani, Karthik},
title = {CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415815},
doi = {10.1145/3379337.3415815},
abstract = {Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems that allow for non-intrusive capturing of user's daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {328–341},
numpages = {14},
keywords = {embodied authoring, end-user programming tool, context-aware application, in-situ authoring, ubiquitous computing, augmented reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415824,
author = {Zhang, Lei and Oney, Steve},
title = {FlowMatic: An Immersive Authoring Tool for Creating Interactive Scenes in Virtual Reality},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415824},
doi = {10.1145/3379337.3415824},
abstract = {Immersive authoring is a paradigm that makes Virtual Reality (VR) application development easier by allowing programmers to create VR content while immersed in the virtual environment. In this paradigm, programmers manipulate programming primitives through direct manipulation and get immediate feedback on their program's state and output. However, existing immersive authoring tools have a low ceiling; their programming primitives are intuitive but can only express a limited set of static relationships between elements in a scene. In this paper, we introduce FlowMatic, an immersive authoring tool that raises the ceiling of expressiveness by allowing programmers to specify reactive behaviors---behaviors that react to discrete events such as user actions, system timers, or collisions. FlowMatic also introduces primitives for programmatically creating and destroying new objects, for abstracting and re-using functionality, and for importing 3D models. Importantly, FlowMatic uses novel visual representations to allow these primitives to be represented directly in VR. We also describe the results of a user study that illustrates the usability advantages of FlowMatic relative to a 2D authoring tool and we demonstrate its expressiveness through several example applications that would be impossible to implement with existing immersive authoring tools. By combining a visual program representation with expressive programming primitives and a natural User Interface (UI) for authoring programs, FlowMatic shows how programmers can build fully interactive virtual experiences with immersive authoring.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {342–353},
numpages = {12},
keywords = {visual programming, virtual reality, immersive authoring},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415846,
author = {Hartmann, Jeremy and DiVerdi, Stephen and Nguyen, Cuong and Vogel, Daniel},
title = {View-Dependent Effects for 360° Virtual Reality Video},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415846},
doi = {10.1145/3379337.3415846},
abstract = {"View-dependent effects'' have parameters that change with the user's view and are rendered dynamically at runtime. They can be used to simulate physical phenomena such as exposure adaptation, as well as for dramatic purposes such as vignettes. We present a technique for adding view-dependent effects to 360 degree video, by interpolating spatial keyframes across an equirectangular video to control effect parameters during playback. An in-headset authoring tool is used to configure effect parameters and set keyframe positions. We evaluate the utility of view-dependent effects with expert 360 degree filmmakers and the perception of the effects with a general audience. Results show that experts find view-dependent effects desirable for their creative purposes and that these effects can evoke novel experiences in an audience.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {354–364},
numpages = {11},
keywords = {visual effects rendering, cinematography, view-dependent effects, 360 degree video, virtual reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415858,
author = {Zhang, Amy X. and Hugh, Grant and Bernstein, Michael S.},
title = {PolicyKit: Building Governance in Online Communities},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415858},
doi = {10.1145/3379337.3415858},
abstract = {The software behind online community platforms encodes a governance model that represents a strikingly narrow set of governance possibilities focused on moderators and administrators. When online communities desire other forms of government, such as ones that take many members? opinions into account or that distribute power in non-trivial ways, communities must resort to laborious manual effort. In this paper, we present PolicyKit, a software infrastructure that empowers online community members to concisely author a wide range of governance procedures and automatically carry out those procedures on their home platforms. We draw on political science theory to encode community governance into policies, or short imperative functions that specify a procedure for determining whether a user-initiated action can execute. Actions that can be governed by policies encompass everyday activities such as posting or moderating a message, but actions can also encompass changes to the policies themselves, enabling the evolution of governance over time. We demonstrate the expressivity of PolicyKit through implementations of governance models such as a random jury deliberation, a multi-stage caucus, a reputation system, and a promotion procedure inspired by Wikipedia's Request for Adminship (RfA) process.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {365–378},
numpages = {14},
keywords = {governance, moderation, toolkit, policy, online communities},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415838,
author = {Prinz, Thomas M. and Graefe, Linda and Pl\"{o}tner, Jan},
title = {Learning from the Past - Do Historical Data Help to Improve Progress Indicators in Web Surveys?},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415838},
doi = {10.1145/3379337.3415838},
abstract = {As a typical part of the interface of web surveys, progress indicators show the degree of completion for participants. These progress indicators influence the dropout and response behavior as various studies suggest. For this reason, the indicator should be chosen carefully. However, calculating the progress in adaptive surveys with many branches is often difficult. Recently related work has provided algorithms for such surveys based on different prediction strategies and has identified the Root Mean Squared Error as a valuable measure to compare different strategies. However, all previously mentioned strategies have shown poor predictions in some cases. In this paper, we present a new strategy which learns from historical data. A simulation study with 10k randomly generated surveys shows its benefits and its limits. As an example of application, we confirm our results of the simulation by comparing different prediction strategies for two large real-world surveys.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {379–390},
numpages = {12},
keywords = {user interface, web survey, learning, prediction strategy, progress indicator},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415865,
author = {Chang, Joseph Chee and Hahn, Nathan and Kittur, Aniket},
title = {Mesh: Scaffolding Comparison Tables for Online Decision Making},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415865},
doi = {10.1145/3379337.3415865},
abstract = {While there is an enormous amount of information online for making decisions such as choosing a product, restaurant, or school, it can be costly for users to synthesize that information into confident decisions. Information for users' many different criteria needs to be gathered from many different sources into a structure where they can be compared and contrasted. The usefulness of each criterion for differentiating potential options can be opaque to users, and evidence such as reviews may be subjective and conflicting, requiring users to interpret each under their personal context. We introduce Mesh, which scaffolds users to iteratively build up a better understanding of both their criteria and options by evaluating evidence gathered across sources in the context of consumer decision-making. Mesh bridges the gap between decision support systems that typically have rigid structures and the fluid and dynamic process of exploratory search, changing the cost structure to provide increasing payoffs with greater user investment. Our lab and field deployment studies found evidence that Mesh significantly reduces the costs of gathering and evaluating evidence and scaffolds decision-making through personalized criteria enabling users to gain deeper insights from data.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {391–405},
numpages = {15},
keywords = {ecommerce, search, note-taking, sensemaking},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415901,
author = {Gong, Jun and Gupta, Aakar and Benko, Hrvoje},
title = {Acustico: Surface Tap Detection and Localization Using Wrist-Based Acoustic TDOA Sensing},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415901},
doi = {10.1145/3379337.3415901},
abstract = {In this paper, we present Acustico, a passive acoustic sensing approach that enables tap detection and 2D tap localization on uninstrumented surfaces using a wrist-worn device. Our technique uses a novel application of acoustic time differences of arrival (TDOA) analysis. We adopt a sensor fusion approach by taking both 'surface waves' (i.e., vibrations through surface) and 'sound waves' (i.e., vibrations through air) into analysis to improve sensing resolution. We carefully design a sensor configuration to meet the constraints of a wristband form factor. We built a wristband prototype with four acoustic sensors, two accelerometers and two microphones. Through a 20-participant study, we evaluated the performance of our proposed sensing technique for tap detection and localization. Results show that our system reliably detects taps with an F1-score of 0.9987 across different environmental noises and yields high localization accuracies with root-mean-square-errors of 7.6mm (X-axis) and 4.6mm (Y-axis) across different surfaces and tapping techniques.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {406–419},
numpages = {14},
keywords = {wrist-based sensing, time differences of arrival (tdoa) analysis, tap detection and localization, passive acoustic},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415823,
author = {Dementyev, Artem and Olwal, Alex and Lyon, Richard F.},
title = {Haptics with Input: Back-EMF in Linear Resonant Actuators to Enable Touch, Pressure and Environmental Awareness},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415823},
doi = {10.1145/3379337.3415823},
abstract = {Today's wearable and mobile devices typically use separate hardware components for sensing and actuation. In this work, we introduce new opportunities for the Linear Resonant Actuator (LRA), which is ubiquitous in such devices due to its capability for providing rich haptic feedback. By leveraging strategies to enable active and passive sensing capabilities with LRAs, we demonstrate their benefits and potential as self-contained I/O devices. Specifically, we use the back-EMF voltage to classify if the LRA is tapped, touched, as well as how much pressure is being applied. The back-EMF sensing is already integrated into many motor and LRA drivers. We developed a passive low-power tap sensing method that uses just 37.7 uA. Furthermore, we developed active touch and pressure sensing, which is low-power, quiet (2 dB), and minimizes vibration. The sensing method works with many types of LRAs. We show applications, such as pressure-sensing side-buttons on a mobile phone. We have also implemented our technique directly on an existing mobile phone's LRA to detect if the phone is handheld or placed on a soft or hard surface. Finally, we show that this method can be used for haptic devices to determine if the LRA makes good contact with the skin. Our approach can add rich sensing capabilities to the ubiquitous LRA actuators without requiring additional sensors or hardware.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {420–429},
numpages = {10},
keywords = {linear resonant actuator, sensing, back-emf, haptic actuator, touch, pressure},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415902,
author = {Tasnim Oshim, Md. Farhan and Killingback, Julian and Follette, Dave and Peng, Huaishu and Rahman, Tauhidur},
title = {MechanoBeat: Monitoring Interactions with Everyday Objects Using 3D Printed Harmonic Oscillators and Ultra-Wideband Radar},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415902},
doi = {10.1145/3379337.3415902},
abstract = {In this paper we present MechanoBeat, a 3D printed mechanical tag that oscillates at a unique frequency upon user interaction. With the help of an ultra-wideband (UWB) radar array, MechanoBeat can unobtrusively monitor interactions with both stationary and mobile objects. MechanoBeat consists of small, scalable, and easy-to-install tags that do not require any batteries, silicon chips, or electronic components. Tags can be produced using commodity desktop 3D printers with cheap materials. We develop an efficient signal processing and deep learning method to locate and identify tags using only the signals reflected from the tag vibrations. MechanoBeat is capable of detecting simultaneous interactions with high accuracy, even in noisy environments. We leverage UWB radar signals' high penetration property to sense interactions behind walls in a non-line-of-sight (NLOS) scenario. A number of applications using MechanoBeat have been explored and the results have been presented in the paper.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {430–444},
numpages = {15},
keywords = {3d printing, user-object interaction, ultra-wideband radar, mechanical oscillator tag, contactless sensing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415849,
author = {Hartmann, Jeremy and Yeh, Yen-Ting and Vogel, Daniel},
title = {AAR: Augmenting a Wearable Augmented Reality Display with an Actuated Head-Mounted Projector},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415849},
doi = {10.1145/3379337.3415849},
abstract = {Current wearable AR devices create an isolated experience with a limited field of view, vergence-accommodation conflicts, and difficulty communicating the virtual environment to observers. To address these issues and enable new ways to visualize, manipulate, and share virtual content, we introduce Augmented Augmented Reality (AAR) by combining a wearable AR display with a wearable spatial augmented reality projector. To explore this idea, a system is constructed to combine a head-mounted actuated pico projector with a Hololens AR headset. Projector calibration uses a modified structure from motion pipeline to reconstruct the geometric structure of the pan-tilt actuator axes and offsets. A toolkit encapsulates a set of high-level functionality to manage content placement relative to each augmented display and the physical environment. Demonstrations showcase ways to utilize the projected and head-mounted displays together, such as expanding field of view, distributing content across depth surfaces, and enabling bystander collaboration.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {445–458},
numpages = {14},
keywords = {augmented reality, projection mapping, calibration, asymmetric interaction, spatial augmented reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415843,
author = {Jansen, Pascal and Fischbach, Fabian and Gugenheimer, Jan and Stemasov, Evgeny and Frommel, Julian and Rukzio, Enrico},
title = {ShARe: Enabling Co-Located Asymmetric Multi-User Interaction for Augmented Reality Head-Mounted Displays},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415843},
doi = {10.1145/3379337.3415843},
abstract = {Head-Mounted Displays (HMDs) are the dominant form of enabling Virtual Reality (VR) and Augmented Reality (AR) for personal use. One of the biggest challenges of HMDs is the exclusion of people in the vicinity, such as friends or family. While recent research on asymmetric interaction for VR HMDs has contributed to solving this problem in the VR domain, AR HMDs come with similar but also different problems, such as conflicting information in visualization through the HMD and projection. In this work, we propose ShARe, a modified AR HMD combined with a projector that can display augmented content onto planar surfaces to include the outside users (non-HMD users). To combat the challenge of conflicting visualization between augmented and projected content, ShARe visually aligns the content presented through the AR HMD with the projected content using an internal calibration procedure and a servo motor. Using marker tracking, non-HMD users are able to interact with the projected content using touch and gestures. To further explore the arising design space, we implemented three types of applications (collaborative game, competitive game, and external visualization). ShARe is a proof-of-concept system that showcases how AR HMDs can facilitate interaction with outside users to combat exclusion and instead foster rich, enjoyable social interactions.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {459–471},
numpages = {13},
keywords = {head-mounted displays, mixed reality, augmented reality, co-located, asymmetric interaction},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415847,
author = {Wang, Chiu-Hsuan and Yong, Seraphina and Chen, Hsin-Yu and Ye, Yuan-Syun and Chan, Liwei},
title = {HMD Light: Sharing In-VR Experience via Head-Mounted Projector for Asymmetric Interaction},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415847},
doi = {10.1145/3379337.3415847},
abstract = {We present HMD Light, a proof-of-concept Head-Mounted Display (HMD) implementation that reveals the Virtual Reality (VR) user's experience in the physical environment to facilitate communication between VR and external users in a mobile VR context. While previous work externalized the VR user's experience through an on-HMD display, HMD Light places the display into the physical environment to enable larger display and interaction area. This work explores the interaction design space of HMD Light and presents four applications to demonstrate its versatility. Our exploratory user study observed participant pairs experience applications with HMD Light and evaluated usability, accessibility and social presence between users. From the results, we distill design insights for HMD Light and asymmetric VR collaboration.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {472–486},
numpages = {15},
keywords = {mobile virtual reality, virtual reality, multi-user virtual reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415857,
author = {Cui, Wenzhe and Zhu, Suwen and Zhang, Mingrui Ray and Schwartz, H. Andrew and Wobbrock, Jacob O. and Bi, Xiaojun},
title = {JustCorrect: Intelligent Post Hoc Text Correction Techniques on Smartphones},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415857},
doi = {10.1145/3379337.3415857},
abstract = {Correcting errors in entered text is a common task but usually difficult to perform on mobile devices due to tedious cursor navigation steps. In this paper, we present JustCorrect, an intelligent post hoc text correction technique for smartphones. To make a correction, the user simply types the correct text at the end of their current input, and JustCorrect will automatically detect the error and apply the correction in the form of an insertion or a substitution. In this way, manual navigation steps are bypassed, and the correction can be committed with a single tap. We solved two critical problems to support JustCorrect: (1) Correction Algorithm: we propose an algorithm that infers the user's correction intention from the last typed word. (2) Input Modalities: our study revealed that both tap and gesture were suitable input modalities for performing JustCorrect. Based on our findings, we integrated JustCorrect into a soft keyboard. Our user studies show that using JustCorrect reduces the text correction time by 12.8% over the stock Android keyboard and by 9.7% over the "Type, then Correct" text correction technique by Zhang et al. (2019). Overall, JustCorrect complements existing post hoc text correction techniques, making error correction more automatic and intelligent.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {487–499},
numpages = {13},
keywords = {smartphones, text entry, error correction},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415861,
author = {Romat, Hugo and Collins, Christopher and Henry Riche, Nathalie and Pahud, Michel and Holz, Christian and Riddle, Adam and Buxton, Bill and Hinckley, Ken},
title = {Tilt-Responsive Techniques for Digital Drawing Boards},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415861},
doi = {10.1145/3379337.3415861},
abstract = {Drawing boards offer a self-stable work surface that is continuously adjustable. On digital displays, such as the Microsoft Surface Studio, these properties open up a class of techniques that sense and respond to tilt adjustments. Each display posture-whether angled high, low, or somewhere in-between-affords some activities, but not others. Because what is appropriate also depends on the application and task, we explore a range of app-specific transitions between reading vs. writing (annotation), public vs. personal, shared person-space vs. task-space, and other nuances of input and feedback, contingent on display angle. Continuous responses provide interactive transitions tailored to each use-case. We show how a variety of knowledge work scenarios can use sensed display adjustments to drive context-appropriate transitions, as well as technical software details of how to best realize these concepts. A preliminary remote user study suggests that techniques must balance effort required to adjust tilt, versus the potential benefits of a sensed transition.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {500–515},
numpages = {16},
keywords = {micro-mobility, adjustable tilt display, posture, drafting table},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415883,
author = {Schmid, Philippe and Malacria, Sylvain and Cockburn, Andy and Nancel, Mathieu},
title = {Interaction Interferences: Implications of Last-Instant System State Changes},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415883},
doi = {10.1145/3379337.3415883},
abstract = {We study interaction interferences, situations where an unexpected change occurs in an interface immediately before the user performs an action, causing the corresponding input to be misinterpreted by the system. For example, a user tries to select an item in a list, but the list is automatically updated immediately before the click, causing the wrong item to be selected. First, we formally define interaction interferences and discuss their causes from behavioral and system-design perspectives. Then, we report the results of a survey examining users' perceptions of the frequency, frustration, and severity of interaction interferences. We also report a controlled experiment, based on state-of-the-art experimental protocols from neuroscience, that explores the minimum time interval, before clicking, below which participants could not refrain from completing their action. Finally, we discuss our findings and their implications for system design, paving the way for future work.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {516–528},
numpages = {13},
keywords = {movement inhibition, interferences, reaction time},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415860,
author = {Lin, Richard and Ramesh, Rohit and Chi, Connie and Jain, Nikhil and Nuqui, Ryan and Dutta, Prabal and Hartmann, Bj\"{o}rn},
title = {Polymorphic Blocks: Unifying High-Level Specification and Low-Level Control for Circuit Board Design},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415860},
doi = {10.1145/3379337.3415860},
abstract = {Mainstream board-level circuit design tools work at the lowest level of design --- schematics and individual components. While novel tools experiment with higher levels of design, abstraction often comes at the expense of the fine-grained control afforded by low-level tools. In this work, we propose a hardware description language (HDL) approach that supports users at multiple levels of abstraction from broad system architecture to subcircuits and component selection. We extend the familiar hierarchical block diagram with polymorphism to include abstract-typed blocks (e.g., generic resistor supertype) and electronics modeling (i.e., currents and voltages). Such an approach brings the advantages of reusability and encapsulation from object-oriented programming, while addressing the unique needs of electronics designers such as physical correctness verification. We discuss the system design, including fundamental abstractions, the block diagram construction HDL, and user interfaces to inspect and fine-tune the design; demonstrate example designs built with our system; and present feedback from intermediate-level engineers who have worked with our system.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {529–540},
numpages = {12},
keywords = {circuit design, hardware description language (hdl), printed circuit board (pcb) design},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415898,
author = {Zhu, Junyi and Zhu, Yunyi and Cui, Jiaming and Cheng, Leon and Snowden, Jackson and Chounlakone, Mark and Wessely, Michael and Mueller, Stefanie},
title = {MorphSensor: A 3D Electronic Design Tool for Reforming Sensor Modules},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415898},
doi = {10.1145/3379337.3415898},
abstract = {MorphSensor is a 3D electronic design tool that enables designers to morph existing sensor modules of pre-defined two-dimensional shape into free-form electronic component arrangements that better integrate with the three-dimensional shape of a physical prototype. MorphSensor builds onto existing sensor module schematics that already define the electronic components and the wiring required to build the sensor. Since MorphSensor maintains the wire connections throughout the editing process, the sensor remains fully functional even when designers change the electronic component layout on the prototype geometry. We detail the MorphSensor editor that supports designers in re-arranging the electronic components, and discuss a fabrication pipeline based on customized PCB footprints for making the resulting freeform sensor. We then demonstrate the capabilities of our system by morphing a range of sensor modules of different complexity and provide a technical evaluation of the quality of the resulting free-form sensors.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {541–553},
numpages = {13},
keywords = {electronic design, personal fabrication, interactive devices, integrating form and function},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415840,
author = {Choi, Youngkyung and Ryu, Neung and Kim, Myung Jin and Dementyev, Artem and Bianchi, Andrea},
title = {BodyPrinter: Fabricating Circuits Directly on the Skin at Arbitrary Locations Using a Wearable Compact Plotter},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415840},
doi = {10.1145/3379337.3415840},
abstract = {On-body electronics and sensors offer the opportunity to seamlessly augment the human with computing power. Accordingly, numerous previous work investigated methods that exploit conductive materials and flexible substrates to fabricate circuits in the form of wearable devices, stretchable patches, and stickers that can be attached to the skin. For all these methods, the fabrication process involves several manual steps, such as designing the circuit in software, constructing conductive patches, and manually placing these physical patches on the body. In contrast, in this work, we propose to fabricate electronics directly on the skin. We present BodyPrinter, a wearable conductive-ink deposition machine, that prints flexible electronics directly on the body using skin-safe conductive ink. The paper describes our system in detail and, through a series of examples and a technical evaluation, we show how direct on-body fabrication of electronic circuits and sensors can further enhance the human body.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {554–564},
numpages = {11},
keywords = {wearable, skin electronics, on-body fabrication},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415875,
author = {Nishida, Jun and Matsuda, Soichiro and Matsui, Hiroshi and Teng, Shan-Yuan and Liu, Ziwei and Suzuki, Kenji and Lopes, Pedro},
title = {HandMorph: A Passive Exoskeleton That Miniaturizes Grasp},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415875},
doi = {10.1145/3379337.3415875},
abstract = {We engineered an exoskeleton, which we call HandMorph, that approximates the experience of having a smaller grasping range. It uses mechanical links to transmit motion from the wearer's fingers to a smaller hand with five anatomically correct fingers. The result is that HandMorph miniaturizes a wearer's grasping range while transmitting haptic feedback.  Unlike other size-illusions based on virtual reality, HandMorph achieves this in the user's real environment, preserving the user's physical and social contexts. As such, our device can be integrated into the user's workflow, e.g., to allow product designers to momentarily change their grasping range into that of a child while evaluating a toy prototype.  In our first user study, we found that participants perceived objects as larger when wearing HandMorph, which suggests that their size perception was successfully transformed. In our second user study, we assessed the experience of using HandMorph in designing a simple toy trumpet for children. We found that participants felt more confident in their toy design when using HandMorph to validate its ergonomics.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {565–578},
numpages = {14},
keywords = {exoskeleton, haptics, perception, embodied design},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415828,
author = {Strohmeier, Paul and G\"{u}ng\"{o}r, Seref and Herres, Luis and Gudea, Dennis and Fruchard, Bruno and Steimle, J\"{u}rgen},
title = {BARefoot: Generating Virtual Materials Using Motion Coupled Vibration in Shoes},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415828},
doi = {10.1145/3379337.3415828},
abstract = {Many features of materials can be experienced through tactile cues, even using one's feet. For example, one can easily distinguish between moss and stone without looking at the ground. However, this type of material experience is largely not supported in AR and VR applications. We present bARefoot, a prototype shoe providing tactile impulses tightly coupled to motor actions. This enables generating virtual material experiences such as compliance, elasticity, or friction. To explore the parameter space of such sensorimotor coupled vibrations, we present a design tool enabling rapid design of virtual materials. We report initial explorations to increase understanding of how parameters can be optimized for generating compliance, and to examine the effect of dynamic parameters on material experiences. Finally, we present a series of use cases that demonstrate the potential of bARefoot for VR and AR.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {579–593},
numpages = {15},
keywords = {haptic feedback, material experiences, augmented reality, virtual reality, haptic rendering, body-based interaction, shoes, wearable computing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415589,
author = {Langerak, Thomas and Z\'{a}rate, Juan Jos\'{e} and Lindlbauer, David and Holz, Christian and Hilliges, Otmar},
title = {Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415589},
doi = {10.1145/3379337.3415589},
abstract = {We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {594–606},
numpages = {13},
keywords = {electromagnets, haptic feedback, electromagnetic sensing, mixed reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415834,
author = {Lerner, Sorin},
title = {Focused Live Programming with Loop Seeds},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415834},
doi = {10.1145/3379337.3415834},
abstract = {Live programming is a paradigm in which the programmer can visualize the runtime values of the program each time the program changes. The promise of live programming depends on using test cases to run the program and thereby provide these runtime values. In this paper we show that in some situations test cases are insufficient in a fundamental way, in that there are no test inputs that can drive certain incomplete loops to produce useful data, a problem we call the loop-datavoid problem. The problem stems from the fact that useful data inside the loop might only be produced after the loop has been fully written. To solve this problem, we propose a paradigm called Focused Live Programming with Loop Seeds, in which the programmer provides hypothetical values to start a loop iteration, and then the programming environment focuses the live visualization on this hypothetical loop iteration. We introduce the loop-datavoid problem, present our proposed solution, explain it in detail, and then present the results of a user study.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {607–613},
numpages = {7},
keywords = {debugging, live programming, program testing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415869,
author = {Ferdowsifard, Kasra and Ordookhanians, Allen and Peleg, Hila and Lerner, Sorin and Polikarpova, Nadia},
title = {Small-Step Live Programming by Example},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415869},
doi = {10.1145/3379337.3415869},
abstract = {Live programming is a paradigm in which the programming environment continually displays runtime values. Program synthesis is a technique that can generate programs or program snippets from examples. deltextThis paper presents a new programming paradigm called Synthesis-Aided Live Programming that combines these two prior ideas in a synergistic way. When using Synthesis-Aided Live Programming, programmers can change the runtime values displayed by the live addtextPrevious works that combine the two have taken a holistic approach to the way examples describe the behavior of functions and programs. This paper presents a new programming paradigm called Small-Step Live Programming by Example that lets the user apply Programming by Example locally. When using Small-Step Live Programming by Example, programmers can change the runtime values displayed by the live visualization to generate local program snippets. % Live programming and program % synthesis work perfectly together because the live programming environment % reifies values, which makes it easy for programmers to provide the examples % needed by the synthesizer. We implemented this new paradigm in a tool called toolname, and performed a user study on $13$ programmers. Our study finds that Small-Step Live Programming by Example with toolname helps users solve harder problems faster, and that for certain types of queries, users prefer it to searching the web. Additionally, we identify the usersynthgap, in which users' mental models of the tool do not match its ability, and needs to be taken into account in the design of future synthesis tools.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {614–626},
numpages = {13},
keywords = {live programming, program synthesis},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415900,
author = {Zhang, Tianyi and Lowmanstone, London and Wang, Xinyu and Glassman, Elena L.},
title = {Interactive Program Synthesis by Augmented Examples},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415900},
doi = {10.1145/3379337.3415900},
abstract = {Programming-by-example (PBE) has become an increasingly popular component in software development tools, human-robot interaction, and end-user programming. A long-standing challenge in PBE is the inherent ambiguity in user-provided examples. This paper presents an interaction model to disambiguate user intent and reduce the cognitive load of understanding and validating synthesized programs. Our model provides two types of augmentations to user-given examples: 1) semantic augmentation where a user can specify how different aspects of an example should be treated by a synthesizer via light-weight annotations, and 2) data augmentation where the synthesizer generates additional examples to help the user understand and validate synthesized programs. We implement and demonstrate this interaction model in the domain of regular expressions, which is a popular mechanism for text processing and data wrangling and is often considered hard to master even for experienced programmers. A within-subjects user study with twelve participants shows that, compared with only inspecting and annotating synthesized programs, interacting with augmented examples significantly increases the success rate of finishing a programming task with less time and increases users? confidence of synthesized programs.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {627–648},
numpages = {22},
keywords = {program synthesis, example augmentation, disambiguation},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415829,
author = {Wu, Te-Yen and Tan, Lu and Zhang, Yuji and Seyed, Teddy and Yang, Xing-Dong},
title = {Capacitivo: Contact-Based Object Recognition on Interactive Fabrics Using Capacitive Sensing},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415829},
doi = {10.1145/3379337.3415829},
abstract = {We present Capacitivo, a contact-based object recognition technique developed for interactive fabrics, using capacitive sensing. Unlike prior work that has focused on metallic objects, our technique recognizes non-metallic objects such as food, different types of fruits, liquids, and other types of objects that are often found around a home or in a workplace. To demonstrate our technique, we created a prototype composed of a 12 x 12 grid of electrodes, made from conductive fabric attached to a textile substrate. We designed the size and separation between the electrodes to maximize the sensing area and sensitivity. We then used a 10-person study to evaluate the performance of our sensing technique using 20 different objects, which yielded a 94.5% accuracy rate. We conclude this work by presenting several different application scenarios to demonstrate unique interactions that are enabled by our technique on fabrics.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {649–661},
numpages = {13},
keywords = {interactive fabrics, object recognition, capacitive sensing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415886,
author = {Wu, Tony and Fukuhara, Shiho and Gillian, Nicholas and Sundara-Rajan, Kishore and Poupyrev, Ivan},
title = {ZebraSense: A Double-Sided Textile Touch Sensor for Smart Clothing},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415886},
doi = {10.1145/3379337.3415886},
abstract = {ZebraSense is a novel dual-sided woven touch sensor that can recognize and differentiate interactions on the top and bottom surfaces of the sensor. ZebraSense is based on an industrial multi-layer textile weaving technique, yet it enables a novel capacitive sensing paradigm, where each sensing element contributes to touch detection on both surfaces of the sensor simultaneously. Unlike the common "sensor sandwich" approach used in previous work, ZebraSense inherently minimizes the number of sensing elements, which drastically simplifies both sensor construction and its integration into soft goods, while preserving maximum sensor resolution. The experimental evaluation confirmed the validity of our approach and demonstrated that ZebraSense is a reliable, efficient, and accurate solution for detecting user gestures in various dual-sided interaction scenarios, allowing for new use cases in smart apparel, home decoration, toys, and other textile objects.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {662–674},
numpages = {13},
keywords = {touch interfaces, wearable computing, smart apparel, dual-sided interaction, digital textiles, capacitive sensing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415888,
author = {Preindl, Thomas and Honnet, Cedric and Pointner, Andreas and Aigner, Roland and Paradiso, Joseph A. and Haller, Michael},
title = {Sonoflex: Embroidered Speakers Without Permanent Magnets},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415888},
doi = {10.1145/3379337.3415888},
abstract = {We present Sonoflex, a thin-form, embroidered dynamic speaker made without using a permanent magnet. Our design consists of two flat spiral coils, stacked on top of each other, and is based on an isolated, thin (0.15 mm) enameled copper wire. Our approach allows for thin, lightweight, and textile speakers and does not require high voltage as in electrostatic speakers. We show how the speaker can be designed and fabricated and evaluate its acoustic properties as a function of manufacturing parameters (size, turn counts, turn spacing, and substrate materials). The experiment results revealed that we can produce audible sound with a broad frequency range (1.5 kHz - 20 kHz) with the embroidered speaker with a diameter of 50 mm. We conclude the paper by presenting several applications such as audible notifications and near-ultrasound communication.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {675–685},
numpages = {11},
keywords = {smart textile, textile transducer, embroidered speaker, lightweight speaker, textile loudspeaker, embroidery},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415816,
author = {Richardson, Mark and Durasoff, Matt and Wang, Robert},
title = {Decoding Surface Touch Typing from Hand-Tracking},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415816},
doi = {10.1145/3379337.3415816},
abstract = {We propose a novel text decoding method that enables touch typing on an uninstrumented flat surface. Rather than relying on physical keyboards or capacitive touch, our method takes as input hand motion of the typist, obtained through hand-tracking, and decodes this motion directly into text. We use a temporal convolutional network to represent a motion model that maps the hand motion, represented as a sequence of hand pose features, into text characters. To enable touch typing without the haptic feedback of a physical keyboard, we had to address more erratic typing motion due to drift of the fingers. Thus, we incorporate a language model as a text prior and use beam search to efficiently combine our motion and language models to decode text from erratic or ambiguous hand motion. We collected a dataset of 20 touch typists and evaluated our model on several baselines, including contact-based text decoding and typing on a physical keyboard. Our proposed method is able to leverage continuous hand pose information to decode text more accurately than contact-based methods and an offline study shows parity (73 WPM, 2.38% UER) with typing on a physical keyboard. Our results show that hand-tracking has the potential to enable rapid text entry in mobile environments.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {686–696},
numpages = {11},
keywords = {augmented reality, virtual reality, hand-tracking, text input},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415835,
author = {Tang, Xiao and Hu, Xiaowei and Fu, Chi-Wing and Cohen-Or, Daniel},
title = {GrabAR: Occlusion-Aware Grabbing Virtual Objects in AR},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415835},
doi = {10.1145/3379337.3415835},
abstract = {Existing augmented reality (AR) applications often ignore the occlusion between real hands and virtual objects when incorporating virtual objects in user's views. The challenges come from the lack of accurate depth and mismatch between real and virtual depth. This paper presents GrabAR1, a new approach that directly predicts the real-and-virtual occlusion and bypasses the depth acquisition and inference. Our goal is to enhance AR applications with interactions between hand (real) and grabbable objects (virtual). With paired images of hand and object as inputs, we formulate a compact deep neural network that learns to generate the occlusion mask. To train the network, we compile a large dataset, including synthetic data and real data. We then embed the trained network in a prototyping AR system to support real-time grabbing of virtual objects. Further, we demonstrate the performance of our method on various virtual objects, compare our method with others through two user studies, and showcase a rich variety of interaction scenarios, in which we can use bare hand to grab virtual objects and directly manipulate them.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {697–708},
numpages = {12},
keywords = {occlusion, neural network, augmented reality, interaction},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415832,
author = {McIntosh, Jess and Zajac, Hubert Dariusz and Stefan, Andreea Nicoleta and Bergstr\"{o}m, Joanna and Hornb\ae{}k, Kasper},
title = {Iteratively Adapting Avatars Using Task-Integrated Optimisation},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415832},
doi = {10.1145/3379337.3415832},
abstract = {Virtual Reality allows users to embody avatars that do not match their real bodies. Earlier work has selected changes to the avatar arbitrarily and it therefore remains unclear how to change avatars to improve users' performance. We propose a systematic approach for iteratively adapting the avatar to perform better for a given task based on users' performance. The approach is evaluated in a target selection task, where the forearms of the avatar are scaled to improve performance. A comparison between the optimised and real arm lengths shows a significant reduction in average tapping time by 18.7%, for forearms multiplied in length by 5.6. Additionally, with the adapted avatar, participants moved their real body and arms significantly less, and subjective measures show reduced physical demand and frustration. In a second study, we modify finger lengths for a linear tapping task to achieve a better performing avatar, which demonstrates the generalisability of the approach.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {709–721},
numpages = {13},
keywords = {optimisation, avatar adaptation, virtual reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415845,
author = {Xia, Haijun},
title = {Crosspower: Bridging Graphics and Linguistics},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415845},
doi = {10.1145/3379337.3415845},
abstract = {Despite the ubiquity of direct manipulation techniques available in computer-aided design applications, creating digital content remains a tedious and indirect task. This is because applications require users to perform numerous low-level editing operations rather than allowing them to directly indicate high-level design goals. Yet, the creation of graphic content, such as videos, animations, and presentations often begins with a description of design goals in natural language, such as screenplays, scripts, outlines. Therefore, there is an opportunity for language-oriented authoring, i.e., leveraging the information found in the structure of a language to facilitate the creation of graphic content. We present a systematic exploration of the identification, graphic description, and interaction with various linguistic structures to assist in the creation of visual content. The prototype system, Crosspower, and its proposed interaction techniques, enables content creators to indicate and customize their desired visual content in a flexible and direct manner.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {722–734},
numpages = {13},
keywords = {reification, text-based editing, language-oriented authoring, natural language processing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415882,
author = {Xia, Haijun and Jacobs, Jennifer and Agrawala, Maneesh},
title = {Crosscast: Adding Visuals to Audio Travel Podcasts},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415882},
doi = {10.1145/3379337.3415882},
abstract = {Audio travel podcasts are a valuable source of information for travelers. Yet, travel is, in many ways, a visual experience and the lack of visuals in travel podcasts can make it difficult for listeners to fully understand the places being discussed. We present Crosscast: a system for automatically adding visuals to audio travel podcasts. Given an audio travel podcast as input, Crosscast uses natural language processing and text mining to identify geographic locations and descriptive keywords within the podcast transcript. Crosscast then uses these locations and keywords to automatically select relevant photos from online repositories and synchronizes their display to align with the audio narration. In a user evaluation, we find that 85.7% of the participants preferred Crosscast generated audio-visual travel podcasts compared to audio-only travel podcasts.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {735–746},
numpages = {12},
keywords = {transcript-based editing, audio-visual content, travel},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415864,
author = {Pavel, Amy and Reyes, Gabriel and Bigham, Jeffrey P.},
title = {Rescribe: Authoring and Automatically Editing Audio Descriptions},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415864},
doi = {10.1145/3379337.3415864},
abstract = {Audio descriptions make videos accessible to those who cannot see them by describing visual content in audio. Producing audio descriptions is challenging due to the synchronous nature of the audio description that must fit into gaps of other video content. An experienced audio description author will produce content that fits narration necessary to understand, enjoy, or experience the video content into the time available. This can be especially tricky for novices to do well. In this paper, we introduce a tool, Rescribe, that helps authors create and refine their audio descriptions. Using Rescribe, authors first create a draft of all the content they would like to include in the audio description. Rescribe then uses a dynamic programming approach to optimize between the length of the audio description, available automatic shortening approaches, and source track lengthening approaches. Authors can iteratively visualize and refine the audio descriptions produced by Rescribe, working in concert with the tool. We evaluate the effectiveness of Rescribe through interviews with blind and visually impaired audio description users who give feedback on Rescribe results. In addition, we invite novice users to create audio descriptions with Rescribe and another tool, finding that users produce audio descriptions with fewer placement errors using Rescribe.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {747–759},
numpages = {13},
keywords = {blind and low vision, paraphrasing, media editing, video, accessibility, audio descriptions, automatic summarization, nlp},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415884,
author = {Peng, Mengqi and Wei, Li-yi and Kazi, Rubaiat Habib and Kim, Vladimir G.},
title = {Autocomplete Animated Sculpting},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415884},
doi = {10.1145/3379337.3415884},
abstract = {Keyframe-based sculpting provides unprecedented freedom to author animated organic models, which can be difficult to create with other methods such as simulation, scripting, and rigging. However, sculpting animated objects can require significant artistic skill and manual labor, even more so than sculpting static 3D shapes or drawing 2D animations, which are already quite challenging.We present a keyframe-based animated sculpting system with the capability to autocomplete user editing under a simple and intuitive brushing interface. Similar to current desktop sculpting and VR brushing tools, users can brush surface details and volume structures. Meanwhile, our system analyzes their workflows and predicts what they might do in the future, both spatially and temporally. Users can accept or ignore these suggestions and thus maintain full control. We propose the first interactive suggestive keyframe sculpting system, specifically for spatio-temporal repetitive tasks, including low-level spatial details and high-level brushing structures across multiple frames. Our key ideas include a deformation-based optimization framework to analyze recorded workflows and synthesize predictions, and a semi-causal global similarity measurement to support flexible brushing stroke sequences and complex shape changes. Our system supports a variety of shape and motion styles, including those difficult to achieve via existing animation systems, such as topological changes that cannot be accomplished via simple rig-based deformations and stylized physically-implausible motions that cannot be simulated. We evaluate our system via a pilot user study that demonstrates the effectiveness of our system.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {760–777},
numpages = {18},
keywords = {sculpting, user interface, clone, modeling, workflow, autocomplete, animation},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415877,
author = {Abdrashitov, Rinat and Chevalier, Fanny and Singh, Karan},
title = {Interactive Exploration and Refinement of Facial Expression Using Manifold Learning},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415877},
doi = {10.1145/3379337.3415877},
abstract = {Posing expressive 3D faces is extremely challenging. Typical facial rigs have upwards of 30 controllable parameters, that while anatomically meaningful, are hard to use due to redundancy of expression, unrealistic configurations, and many semantic and stylistic correlations between the parameters. We propose a novel interface for rapid exploration and refinement of static facial expressions, based on a data-driven face manifold of natural expressions. Rapidly explored face configurations are interactively projected onto this manifold of meaningful expressions. These expressions can then be refined using a 2D embedding of nearby faces, both on and off the manifold. Our validation is fourfold: we show expressive face creation using various devices; we verify that our learnt manifold transcends its training face, to expressively control very different faces; we perform a crowd-sourced study to evaluate the quality of manifold face expressions; and we report on a usability study that shows our approach is an effective interactive tool to author facial expression.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {778–790},
numpages = {13},
keywords = {manifold learning, blendshape, 3d face modeling},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415822,
author = {Liu, Jingyuan and Fu, Hongbo and Tai, Chiew-Lan},
title = {PoseTween: Pose-Driven Tween Animation},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415822},
doi = {10.1145/3379337.3415822},
abstract = {Augmenting human action videos with visual effects often requires professional tools and skills. To make this more accessible by novice users, existing attempts have focused on automatically adding visual effects to faces and hands, or let virtual objects strictly track certain body parts, resulting in rigid-looking effects. We present PoseTween, an interactive system that allows novice users to easily add vivid virtual objects with their movement interacting with a moving subject in an input video. Our key idea is to leverage the motion of the subject to create pose-driven tween animations of virtual objects. With our tool, a user only needs to edit the properties of a virtual object with respect to the subject's movement at keyframes, and the object is associated with certain body parts automatically. The properties of the object at intermediate frames are then determined by both the body movement and the interpolated object keyframe properties, producing natural object movements and interactions with the subject. We design a user interface to facilitate editing of keyframes and previewing animation results. Our user study shows that PoseTween significantly requires less editing time and fewer keyframes than using the traditional tween animation in making pose-driven tween animations for novice users.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {791–804},
numpages = {14},
keywords = {interface., tween animation, interpolation, human pose},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415868,
author = {Wang, Chiu-Hsuan and Tsai, Chia-En and Yong, Seraphina and Chan, Liwei},
title = {Slice of Light: Transparent and Integrative Transition Among Realities in a Multi-HMD-User Environment},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415868},
doi = {10.1145/3379337.3415868},
abstract = {This work presents Slice of Light, a visualization design created to enhance transparency and integrative transition between realities of Head-Mounted Display (HMD) users sharing the same physical environment. Targeted at reality-guests, Slice of Light's design enables guests to view other HMD users' interactions contextualized in their own virtual environments while allowing the guests to navigate among these virtual environments. In this paper, we detail our visualization design and the implementation. We demonstrate Slice of Light with a block-world construction scenario that involves a multi-HMD-user environment. VR developer and HCI expert participants were recruited to evaluate the scenario, and responded positively to Slice of Light. We discuss their feedback, our design insights, and the limitations of this work.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {805–817},
numpages = {13},
keywords = {transition interface, virtual reality, spatial augmented reality, multi-realities environment},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415878,
author = {Hayatpur, Devamardeep and Xia, Haijun and Wigdor, Daniel},
title = {DataHop: Spatial Data Exploration in Virtual Reality},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415878},
doi = {10.1145/3379337.3415878},
abstract = {Virtual reality has recently been adopted for use within the domain of visual analytics because it can provide users with an endless workspace within which they can be actively engaged and use their spatial reasoning skills for data analysis. However, virtual worlds need to utilize layouts and organizational schemes that are meaningful to the user and beneficial for data analysis. This paper presents DataHop, a novel visualization system that enables users to lay out their data analysis steps in a virtual environment. With a Filter, a user can specify the modification they wish to perform on one or more input data panels (i.e., containers of points), along with where output data panels should be placed in the virtual environment. Using this simple tool, highly intricate and useful visualizations may be generated and traversed by harnessing a user's spatial abilities. An exploratory study conducted with six virtual reality users evaluated the usability, affordances, and performance of DataHop for data analysis tasks, and found that spatially mapping one's workflow can be beneficial when exploring multidimensional datasets.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {818–828},
numpages = {11},
keywords = {immersive data visualization, spatial skills},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415881,
author = {Du, Ruofei and Turner, Eric and Dzitsiuk, Maksym and Prasso, Luca and Duarte, Ivo and Dourgarian, Jason and Afonso, Joao and Pascoal, Jose and Gladstone, Josh and Cruces, Nuno and Izadi, Shahram and Kowdle, Adarsh and Tsotsos, Konstantine and Kim, David},
title = {DepthLab: Real-Time 3D Interaction with Depth Maps for Mobile Augmented Reality},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415881},
doi = {10.1145/3379337.3415881},
abstract = {Mobile devices with passive depth sensing capabilities are ubiquitous, and recently active depth sensors have become available on some tablets and AR/VR devices. Although real-time depth data is accessible, its rich value to mainstream AR applications has been sorely under-explored. Adoption of depth-based UX has been impeded by the complexity of performing even simple operations with raw depth data, such as detecting intersections or constructing meshes. In this paper, we introduce DepthLab, a software library that encapsulates a variety of depth-based UI/UX paradigms, including geometry-aware rendering (occlusion, shadows), surface interaction behaviors (physics-based collisions, avatar path planning), and visual effects (relighting, 3D-anchored focus and aperture effects). We break down the usage of depth into localized depth, surface depth, and dense depth, and describe our real-time algorithms for interaction and rendering tasks. We present the design process, system, and components of DepthLab to streamline and centralize the development of interactive depth features. We have open-sourced our software at https://github.com/googlesamples/arcore-depth-lab to external developers, conducted performance evaluation, and discussed how DepthLab can accelerate the workflow of mobile AR designers and developers. With DepthLab we aim to help mobile developers to effortlessly integrate depth into their AR experiences and amplify the expression of their creative vision.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {829–843},
numpages = {15},
keywords = {depth map, mobile ar, interactive 3d graphics, real time, interaction, gpu, arcore, rendering, augmented reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415896,
author = {Yamanaka, Shota and Usuba, Hiroki and Takahashi, Haruki and Miyashita, Homei},
title = {Servo-Gaussian Model to Predict Success Rates in Manual Tracking: Path Steering and Pursuit of 1D Moving Target},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415896},
doi = {10.1145/3379337.3415896},
abstract = {We propose a Servo-Gaussian model to predict success rates in continuous manual tracking tasks. Two tasks were conducted to validate this model: path steering and pursuit of a 1D moving target. We hypothesized that (1) hand movements follow the servo-mechanism model, (2) submovement endpoints form a bivariate Gaussian distribution, thus enabling us to predict the success rate at which a submovement endpoint falls inside the tolerance, and (3) the success rate for a whole trial can be predicted if the number of submovements is known. The cross-validation showed R^2&gt;0.92 and MAE&lt;4.9% for steering and R^2&gt;0.95 and MAE&lt;6.5% for pursuit tasks. These results demonstrate that our proposed model delivers high prediction accuracy even for unknown datasets.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {844–857},
numpages = {14},
keywords = {servo-mechanism model, success rate prediction, moving targets, steering law, manual tracking},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415871,
author = {Ko, Yu-Jung and Zhao, Hang and Kim, Yoonsang and Ramakrishnan, IV and Zhai, Shumin and Bi, Xiaojun},
title = {Modeling Two Dimensional Touch Pointing},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415871},
doi = {10.1145/3379337.3415871},
abstract = {Modeling touch pointing is essential to touchscreen interface development and research, as pointing is one of the most basic and common touch actions users perform on touchscreen devices. Finger-Fitts Law [4] revised the conventional Fitts? law into a 1D (one-dimensional) pointing model for finger touch by explicitly accounting for the fat finger ambiguity (absolute error) problem which was unaccounted for in the original Fitts? law. We generalize Finger-Fitts law to 2D touch pointing by solving two critical problems. First, we extend two of the most successful 2D Fitts law forms to accommodate finger ambiguity. Second, we discovered that using nominal target width and height is a conceptually simple yet effective approach for defining amplitude and directional constraints for 2D touch pointing across different movement directions. The evaluation shows our derived 2D Finger-Fitts law models can be both principled and powerful. Specifically, they outperformed the existing 2D Fitts? laws, as measured by the regression coefficient and model selection information criteria (e.g., Akaike Information Criterion) considering the number of parameters. Finally, 2D Finger-Fitts laws also advance our understanding of touch pointing and thereby serve as the basis for touch interface designs.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {858–868},
numpages = {11},
keywords = {fitts law, finger input, pointing models},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415833,
author = {Antoine, Axel and Nancel, Mathieu and Ge, Ella and Zheng, Jingjie and Zolghadr, Navid and Casiez, G\'{e}ry},
title = {Modeling and Reducing Spatial Jitter Caused by Asynchronous Input and Output Rates},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415833},
doi = {10.1145/3379337.3415833},
abstract = {Jitter in interactive systems occurs when visual feedback is perceived as unstable or trembling even though the input signal is smooth or stationary. It can have multiple causes such as sensing noise, or feedback calculations introducing or exacerbating sensing imprecisions. Jitter can however occur even when each individual component of the pipeline works perfectly, as a result of the differences between the input frequency and the display refresh rate. This asynchronicity can introduce rapidly-shifting latencies between the rendered feedbacks and their display on screen, which can result in trembling cursors or viewports. % This paper contributes a better understanding of this particular type of jitter. We first detail the problem from a mathematical standpoint, from which we develop a predictive model of jitter amplitude as a function of input and output frequencies, and a new metric to measure this spatial jitter. Using touch input data gathered in a study, we developed a simulator to validate this model and to assess the effects of different techniques and settings with any output frequency. The most promising approach, when the time of the next display refresh is known, is to estimate (interpolate or extrapolate) the user's position at a fixed time interval before that refresh. % When input events occur at 125~Hz, as is common in touch screens, we show that an interval of 4 to 6~ms works well for a wide range of display refresh rates. This method effectively cancels most of the jitter introduced by input/output asynchronicity, while introducing minimal imprecision or latency.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {869–881},
numpages = {13},
keywords = {spatial jitter, asynchronicity., input frequency, jitter, noise, resampling, output frequency},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415831,
author = {Nakagaki, Ken and Leong, Joanne and Tappa, Jordan L. and Wilbert, Jo\~{a}o and Ishii, Hiroshi},
title = {HERMITS: Dynamically Reconfiguring the Interactivity of Self-Propelled TUIs with Mechanical Shell Add-Ons},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415831},
doi = {10.1145/3379337.3415831},
abstract = {We introduce HERMITS, a modular interaction architecture for self-propelled Tangible User Interfaces (TUIs) that incorporates physical add-ons, referred to as mechanical shells. The mechanical shell add-ons are intended to be dynamically reconfigured by utilizing the locomotion capability of self-propelled TUIs (e.g. wheeled TUIs, swarm UIs). We developed a proof-of-concept system that demonstrates this novel architecture using two-wheeled robots and a variety of mechanical shell examples. These mechanical shell add-ons are passive physical attatchments that extend the primitive interactivities (e.g. shape, motion and light) of the self-propelled robots.  The paper proposes the architectural design, interactive functionality of HERMITS as well as design primitives for mechanical shells. The paper also introduces the prototype implementation that is based on an off-the-shelf robotic toy with a modified docking mechanism. A range of applications is demonstrated with the prototype to motivate the collective and dynamically reconfigurable capability of the modular architecture, such as an interactive mobility simulation, an adaptive home/desk environment, and a story-telling narrative. Lastly, we discuss the future research opportunity of HERMITS to enrich the interactivity and adaptability of actuated and shape changing TUIs.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {882–896},
numpages = {15},
keywords = {human robot interaction, mechanical shell, swarm user interface, actuated tangible user interface},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415826,
author = {Li, Jiahao and Cui, Meilin and Kim, Jeeeun and Chen, Xiang 'Anthony'},
title = {Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415826},
doi = {10.1145/3379337.3415826},
abstract = {Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair's armrest, a piggy bank can reach out an arm to 'steal' coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such 'transformables' using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design.  We present Romeo -- a design tool for creating transformables to robotically augment objects' default functionalities. Romeo allows users to transform an object into a robotic arm by expressing at a high level what type of task is expected. Users can select which part of the object to be transformed, specify motion points in space for the transformed part to follow and the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. A design session validated this tool where participants used Romeo to accomplish controlled design tasks and to open-endedly create coin-stealing piggy banks by transforming 3D objects of their own choice.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {897–911},
numpages = {15},
keywords = {transformables, design tool, robotic task, generative design},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415894,
author = {Niiyama, Ryuma and Sato, Hiroki and Tsujimura, Kazzmasa and Narumi, Koya and Seong, Young ah and Yamamura, Ryosuke and Kakehi, Yasuaki and Kawahara, Yoshihiro},
title = {Poimo: Portable and Inflatable Mobility Devices Customizable for Personal Physical Characteristics},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415894},
doi = {10.1145/3379337.3415894},
abstract = {Despite the recent growth in popularity of personal mobility devices (e.g., e-scooters and e-skateboards), they still suffer from limited safety and narrow design form factors, due to their rigid structures. On the other hand, inflatable interfaces studied in human-computer interaction can achieve large volume change by simple inflation/deflation. Inflatable structure also offers soft and safe interaction owing to material compliance and diverse fabrication methods that lead to a wide range of forms and aesthetics. In this paper, we propose poimo, a new family of POrtable and Inflatable MObility devices, which consists of inflatable frames, inflatable wheels, and inflatable steering mechanisms made of a mass-manufacturable material called drop-stitch fabric. First, we defined the basic material properties of a drop-stitch inflatable structure that is sufficiently strong to carry a person while simultaneously allowing soft deformation and deflation for storage and portability. We then implemented an interactive design system that can scan the user's desired riding posture to generate a customized personal mobility device and can add the user's shape and color preferences. To demonstrate the custom-design capability and mobility, we designed several 3D models using our system and built physical samples for two basic templates: a motorcycle and a wheelchair. Finally, we conducted an online user study to examine the usability of the design system and share lessons learned for further improvements in the design and fabrication of poimo.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {912–923},
numpages = {12},
keywords = {graphical user interfaces design, personal mobility devices, soft robotics, shape-changing interfaces, inflatable interfaces},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415837,
author = {Park, Chaeyong and Yoon, Jinhyuk and Oh, Seungjae and Choi, Seungmoon},
title = {Augmenting Physical Buttons with Vibrotactile Feedback for Programmable Feels},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415837},
doi = {10.1145/3379337.3415837},
abstract = {Physical buttons provide clear haptic feedback when pressed and released, but their responses are unvarying. Physical buttons can be powered by force actuators to produce unlimited click sensations, but the cost is substantial. An alternative can be augmenting physical buttons with simple and inexpensive vibration actuators. When pushed, an augmented button generates a vibration overlayed on the button's original kinesthetic response, under the general framework of haptic augmented reality. We explore the design space of augmented buttons while changing vibration frequency, amplitude, duration, and envelope. We then visualize the perceptual structure of augmented buttons by estimating a perceptual space for 7 physical buttons and 40 augmented buttons. Their sensations are also assessed against adjectives, and results are mapped into the perceptual space to identify meaningful perceptual dimensions. Our results contribute to understanding the benefits and limitations of programmable vibration-augmented physical buttons with emphasis on their feels.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {924–937},
numpages = {14},
keywords = {multimodal, button, augmented reality, haptics, vibrotactile},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415850,
author = {Shim, Youngbo Aram and Park, Keunwoo and Lee, Sangyoon and Son, Jeongmin and Woo, Taeyun and Lee, Geehyuk},
title = {FS-Pad: Video Game Interactions Using Force Feedback Gamepad},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415850},
doi = {10.1145/3379337.3415850},
abstract = {Force feedback has not been fully explored in modern gaming environments where a gamepad is the main interface. We developed various game interaction scenarios where force feedback through the thumbstick of the gamepad can be effective, and categorized them into five themes. We built a haptic device and control system that can support all presented interactions. The resulting device, FS-Pad, has sufficient fidelity to be used as a haptic game interaction design tool. To verify the presented interactions and effectiveness of the FS-Pad, we conducted a user study with game players, developers, and designers. The subjects used an FS-Pad while playing a demo game and were then interviewed. Their feedback revealed the actual needs for the presented interactions as well as insight into the potential design of game interactions when applying FS-Pad.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {938–950},
numpages = {13},
keywords = {thumbstick., game interaction, gamepad, force feedback, haptic feedback, video game},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415593,
author = {Langerak, Thomas and Z\'{a}rate, Juan Jos\'{e} and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar},
title = {Optimal Control for Electromagnetic Haptic Guidance Systems},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415593},
doi = {10.1145/3379337.3415593},
abstract = {We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {951–965},
numpages = {15},
keywords = {optimal control, haptic devices, computational interaction},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415813,
author = {Setlur, Vidya and Hoque, Enamul and Kim, Dae Hyun and Chang, Angel X.},
title = {Sneak Pique: Exploring Autocompletion as a Data Discovery Scaffold for Supporting Visual Analysis},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415813},
doi = {10.1145/3379337.3415813},
abstract = {Natural language interaction has evolved as a useful modality to help users explore and interact with their data during visual analysis. Little work has been done to explore how autocompletion can help with data discovery while helping users formulate analytical questions. We developed a system called system as a design probe to better understand the usefulness of autocompletion for visual analysis. We ran three Mechanical Turk studies to evaluate user preferences for various text- and visualization widget-based autocompletion design variants for helping with partial search queries. Our findings indicate that users found data previews to be useful in the suggestions. Widgets were preferred for previewing temporal, geospatial, and numerical data while text autocompletion was preferred for categorical and hierarchical data. We conducted an exploratory analysis of our system implementing this specific subset of preferred autocompletion variants. Our insights regarding the efficacy of these autocompletion suggestions can inform the future design of natural language interfaces supporting visual analysis.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {966–978},
numpages = {13},
keywords = {natural language interaction, visual analysis, autocompletion, data preview},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415592,
author = {Fraser, C. Ailie and Markel, Julia M. and Basa, N. James and Dontcheva, Mira and Klemmer, Scott},
title = {ReMap: Lowering the Barrier to Help-Seeking with Multimodal Search},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415592},
doi = {10.1145/3379337.3415592},
abstract = {People often seek help online while using complex software. Currently, information search takes users' attention away from the task at hand by creating a separate search task. This paper investigates how multimodal interaction can make in-task help-seeking easier and faster. We introduce ReMap, a multimodal search interface that helps users find video assistance while using desktop and web applications. Users can speak search queries, add application-specific terms deictically (e.g., "how to erase this"), and navigate search results via speech, all without taking their hands (or mouse) off their current task. Thirteen participants who used ReMap in the lab found that it helped them stay focused on their task while simultaneously searching for and using learning videos. Users' experiences with ReMap also raised a number of important challenges with implementing system-wide context-aware multimodal assistance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {979–986},
numpages = {8},
keywords = {multimodal search, deixis, contextual search, speech},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415887,
author = {Kim, Yoonji and Lee, Hyein and Prasad, Ramkrishna and Je, Seungwoo and Choi, Youngkyung and Ashbrook, Daniel and Oakley, Ian and Bianchi, Andrea},
title = {SchemaBoard: Supporting Correct Assembly of Schematic Circuits Using Dynamic In-Situ Visualization},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415887},
doi = {10.1145/3379337.3415887},
abstract = {Assembling circuits on breadboards using reference designs is a common activity among makers. While tools like Fritzing offer a simplified visualization of how components and wires are connected, such pictorial depictions of circuits are rare in formal educational materials and the vast bulk of online technical documentation. Electronic schematics are more common but are perceived as challenging and confusing by novice makers. To improve access to schematics, we propose SchemaBoard, a system for assisting makers in assembling and inspecting circuits on breadboards from schematic source materials. SchemaBoard uses an LED matrix integrated underneath a working breadboard to visualize via light patterns where and how components should be placed, or to highlight elements of circuit topology such as electrical nets and connected pins. This paper presents a formative study with 16 makers, the SchemaBoard system, and a summative evaluation with an additional 16 users. Results indicate that SchemaBoard is effective in reducing both the time and the number of errors associated with building a circuit based on a reference schematic, and for inspecting the circuit for correctness after its assembly.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {987–998},
numpages = {12},
keywords = {breadboard visualization, circuits, physical computing, system},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415880,
author = {Jeong, Yunwoo and Kim, Han-Jong and Yun, Gyeongwon and Nam, Tek-Jin},
title = {WIKA: A Projected Augmented Reality Workbench for Interactive Kinetic Art},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415880},
doi = {10.1145/3379337.3415880},
abstract = {Iterative artistic exploration, mechanism building, and interaction programming are essential processes of prototyping interactive kinetic art (IKA). However, scattered tools and interwoven workflows across digital and physical worlds make the task difficult. We present WIKA, an integrated environment supporting the whole creation process of IKA in the form of a layered picture frame in a single workspace. A projected AR system with a mobile device efficiently makes an interactive tabletop. The projected information connected with physical components (e.g. sensors and motors) enables the programming and simulation on the workspace. Physical components are applied from the initial phase of prototyping using an AR plate, and this supports the iterative trial-and-error process by bridging the workflow. A user study shows that WIKA enabled non-experts to create diverse IKA with their ideas. A tangible interaction and projected information enable the iterative and rapid creation. The method that integrates the hardware and software in the physical environment can be applied to other prototyping tools that support the creation of interactive and kinetic elements.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {999–1009},
numpages = {11},
keywords = {interactive kinetic art, tangible interface, augmented information, prototyping, visual programming, augmented reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415819,
author = {Yamaguchi, Masahiro and Mori, Shohei and Mohr, Peter and Tatzgern, Markus and Stanescu, Ana and Saito, Hideo and Kalkofen, Denis},
title = {Video-Annotated Augmented Reality Assembly Tutorials},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415819},
doi = {10.1145/3379337.3415819},
abstract = {We present a system for generating and visualizing interactive 3D Augmented Reality tutorials based on 2D video input, which allows viewpoint control at runtime. Inspired by assembly planning, we analyze the input video using a 3D CAD model of the object to determine an assembly graph that encodes blocking relationships between parts. Using an assembly graph enables us to detect assembly steps that are otherwise difficult to extract from the video, and generally improves object detection and tracking by providing prior knowledge about movable parts. To avoid information loss, we combine the 3D animation with relevant parts of the 2D video so that we can show detailed manipulations and tool usage that cannot be easily extracted from the video. To further support user orientation, we visually align the 3D animation with the real-world object by using texture information from the input video. We developed a presentation system that uses commonly available hardware to make our results accessible for home use and demonstrate the effectiveness of our approach by comparing it to traditional video tutorials.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1010–1022},
numpages = {13},
keywords = {assembly tutorial., retargeting, augmented reality, video label},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415836,
author = {Wei, Tzu-Yun and Tsai, Hsin-Ruey and Liao, Yu-So and Tsai, Chieh and Chen, Yi-Shan and Wang, Chi and Chen, Bing-Yu},
title = {ElastiLinks: Force Feedback between VR Controllers with Dynamic Points of Application of Force},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415836},
doi = {10.1145/3379337.3415836},
abstract = {Force feedback is commonly used to enhance realism in virtual reality (VR). However, current works mainly focus on providing different force types or patterns, but do not investigate how a proper point of application of force (PAF), which means where the resultant force is applied to, affects users' experience. For example, users perceive resistive force without torque when pulling a virtual bow, but with torque when pulling a virtual slingshot. Therefore, we propose a set of handheld controllers, ElastiLinks, to provide force feedback between controllers with dynamic PAFs.A rotatable track on each controller provides a dynamic PAF, and two common types of force feedback, resistive force and impact, are produced by two links, respectively. We performed a force perception study to ascertain users' resistive and impact force level distinguishability between controllers. Based on the results, we conducted another perception study to understand users' distinguishability of PAF offset and rotation differences. Finally, we performed a VR experience study to prove that force feedback with dynamic PAFs enhances VR experience.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1023–1034},
numpages = {12},
keywords = {impact, point of application of force, force feedback, resistive force, haptic feedback, virtual reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415862,
author = {Ryu, Neung and Lee, Woojin and Kim, Myung Jin and Bianchi, Andrea},
title = {ElaStick: A Handheld Variable Stiffness Display for Rendering Dynamic Haptic Response of Flexible Object},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415862},
doi = {10.1145/3379337.3415862},
abstract = {Haptic controllers have an important role in providing rich and immersive Virtual Reality (VR) experiences. While previous works have succeeded in creating handheld devices that simulate dynamic properties of rigid objects, such as weight, shape, and movement, recreating the behavior of flexible objects with different stiffness using ungrounded controllers remains an open challenge. In this paper we present ElaStick, a variable-stiffness controller that simulates the dynamic response resulting from shaking or swinging flexible virtual objects. This is achieved by dynamically changing the stiffness of four custom elastic tendons along a joint that effectively increase and reduce the overall stiffness of a perceived object in 2-DoF. We show that with the proposed mechanism, we can render stiffness with high precision and granularity in a continuous range between 10.8 and 71.5Nmm/degree. We estimate the threshold of the human perception of stiffness with a just-noticeable difference (JND) study and investigate the levels of immersion, realism and enjoyment using a VR application.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1035–1045},
numpages = {11},
keywords = {dynamic force response, virtual reality, stiffness, controller, haptics},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415854,
author = {Kovacs, Robert and Ofek, Eyal and Gonzalez Franco, Mar and Siu, Alexa Fay and Marwecki, Sebastian and Holz, Christian and Sinclair, Mike},
title = {Haptic PIVOT: On-Demand Handhelds in VR},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415854},
doi = {10.1145/3379337.3415854},
abstract = {We present PIVOT, a wrist-worn haptic device that renders virtual objects into the user's hand on demand. Its simple design comprises a single actuated joint that pivots a haptic handle into and out of the user's hand, rendering the haptic sensations of grasping, catching, or throwing an object anywhere in space. Unlike existing hand-held haptic devices and haptic gloves, PIVOT leaves the user's palm free when not in use, allowing users to make unencumbered use of their hand. PIVOT also enables rendering forces acting on the held virtual objects, such as gravity, inertia, or air-drag, by actively driving its motor while the user is firmly holding the handle. When wearing a PIVOT device on both hands, they can add haptic feedback to bimanual interaction, such as lifting larger objects. In our user study, participants (n=12) evaluated the realism of grabbing and releasing objects of different shape and size with mean score 5.19 on a scale from 1 to 7, rated the ability to catch and throw balls in different directions with different velocities (mean=5.5), and verified the ability to render the comparative weight of held objects with 87% accuracy for ~100g increments.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1046–1059},
numpages = {14},
keywords = {virtual reality, vr controller, haptic feedback, haptic proxy},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415821,
author = {de la Torre-Ortiz, Carlos and Spap\'{e}, Michiel M. and Kangassalo, Lauri and Ruotsalo, Tuukka},
title = {Brain Relevance Feedback for Interactive Image Generation},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415821},
doi = {10.1145/3379337.3415821},
abstract = {Brain-computer interfaces (BCIs) are increasingly used to perform simple operations such as a moving a cursor, but have remained of limited use for more complex tasks. In our new approach to BCI, we use brain relevance feedback to control a generative adversarial network (GAN). We obtained EEG data from 31 participants who viewed face images while concentrating on particular facial features. Following, an EEG relevance classifier was trained and propagated as feedback on the latent image representation provided by the GAN. Estimates for individual vectors matching the relevant criteria were iteratively updated to optimize an image generation process towards mental targets. A double-blind evaluation showed high performance (86.26% accuracy) against random feedback (18.71%), and not significantly lower than explicit feedback (93.30%). Furthermore, we show the feasibility of the method with simultaneous task targets demonstrating BCI operation beyond individual task constraints. Thus, brain relevance feedback can validly control a generative model, overcoming a critical limitation of current BCI approaches.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1060–1070},
numpages = {11},
keywords = {generative models, brain-computer interfaces, image search},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415844,
author = {Hohmann, Matthias R. and Konieczny, Lisa and Hackl, Michelle and Wirth, Brian and Zaman, Talha and Enficiaud, Raffi and Grosse-Wentrup, Moritz and Sch\"{o}lkopf, Bernhard},
title = {MYND: Unsupervised Evaluation of Novel BCI Control Strategies on Consumer Hardware},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415844},
doi = {10.1145/3379337.3415844},
abstract = {Neurophysiological laboratory studies are often constraint to immediate geographical surroundings and access to equipment may be temporally restricted. Limitations of ecological validity, scalability, and generalizability of findings pose a significant challenge for the development of brain-computer interfaces (BCIs), which ultimately need to function in any context, on consumer-grade hardware. We introduce MYND: An open-source framework that couples consumer-grade recording hardware with an easy-to-use application for the unsupervised evaluation of BCI control strategies. Subjects are guided through experiment selection, hardware fitting, recording, and data upload in order to self-administer multi-day studies that include neurophysiological recordings and questionnaires at home. As a use case, thirty subjects evaluated two BCI control strategies "Positive memories" and "Music imagery" by using a four-channel electroencephalogram (EEG) with MYND. Neural activity in both control strategies could be decoded with an average offline accuracy of 68.5% and 64.0% across all days.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1071–1084},
numpages = {14},
keywords = {self-supervised study, bci, eeg, unsupervised study, electroencephalography, brain-computer interface, smartphone application},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415852,
author = {Miyashita, Homei},
title = {Taste Display That Reproduces Tastes Measured by a Taste Sensor},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415852},
doi = {10.1145/3379337.3415852},
abstract = {Aiming for the creation and development of taste media, a taste display was developed in this study that can reproduce tastes measured using taste sensors. By performing iontophoresis on five gels, which contain dissolved electrolytes that reproduce the five basic tastes, the quantity of ions that contact the tongue was controlled. A tasteless gel was added, so that the sum of the currents flowing in the six gels could be kept constant, ensuring a uniform amount of stimulation on the tongue. The measured tastes could be successfully reproduced through calibration, in which the indicated taste levels were matched with the taste-sensor measurements. Furthermore, video-editing software was adapted to edit taste information as well as recorded audio and video. In addition, effector and equalizer prototypes were built that can not only reproduce the recorded tastes in their original states but also adjust the tastes to match individual preferences.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1085–1093},
numpages = {9},
keywords = {global selectivity, taste display, taste effects, taste sensor, electric taste},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415820,
author = {Li, Toby Jia-Jun and Chen, Jingya and Xia, Haijun and Mitchell, Tom M. and Myers, Brad A.},
title = {Multi-Modal Repairs of Conversational Breakdowns in Task-Oriented Dialogs},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415820},
doi = {10.1145/3379337.3415820},
abstract = {A major problem in task-oriented conversational agents is the lack of support for the repair of conversational breakdowns. Prior studies have shown that current repair strategies for these kinds of errors are often ineffective due to: (1) the lack of transparency about the state of the system's understanding of the user's utterance; and (2) the system's limited capabilities to understand the user's verbal attempts to repair natural language understanding errors. This paper introduces SOVITE, a new multi-modal speech plus direct manipulation interface that helps users discover, identify the causes of, and recover from conversational breakdowns using the resources of existing mobile app GUIs for grounding. SOVITE displays the system's understanding of user intents using GUI screenshots, allows users to refer to third-party apps and their GUI screens in conversations as inputs for intent disambiguation, and enables users to repair breakdowns using direct manipulation on these screenshots. The results from a remote user study with 10 users using SOVITE in 7 scenarios suggested that SOVITE's approach is usable and effective.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1094–1107},
numpages = {14},
keywords = {conversational interfaces, instructable agents, chatbots, disambiguation, conversational breakdown, gui semantics, breakdown repair, grounding in communication},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415817,
author = {Olwal, Alex and Balke, Kevin and Votintcev, Dmitrii and Starner, Thad and Conn, Paula and Chinh, Bonnie and Corda, Benoit},
title = {Wearable Subtitles: Augmenting Spoken Communication with Lightweight Eyewear for All-Day Captioning},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415817},
doi = {10.1145/3379337.3415817},
abstract = {Mobile solutions can help transform speech and sound into visual representations for people who are deaf or hard-of-hearing (DHH). However, where handheld phones present challenges, head-worn displays (HWDs) could further communication through privately transcribed text, hands-free use, improved mobility, and socially acceptable interactions.  Wearable Subtitles is a lightweight 3D-printed proof-of-concept HWD that explores augmenting communication through sound transcription for a full workday. Using a low-power microcontroller architecture, we enable up to 15 hours of continuous use. We describe a large survey (n=501) and three user studies with 24 deaf/hard-of-hearing participants which inform our development and help us refine our prototypes. Our studies and prior research identify critical challenges for the adoption of HWDs which we address through extended battery life, lightweight and balanced mechanical design (54 g), fitting options, and form factors that are compatible with current social norms.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1108–1120},
numpages = {13},
keywords = {head-worn displays, hearing accessibility, assistive technology, wearables, all-day, low-power system, captions},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415588,
author = {Ahuja, Karan and Kong, Andy and Goel, Mayank and Harrison, Chris},
title = {Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415588},
doi = {10.1145/3379337.3415588},
abstract = {Future homes and offices will feature increasingly dense ecosystems of IoT devices, such as smart lighting, speakers, and domestic appliances. Voice input is a natural candidate for interacting with out-of-reach and often small devices that lack full-sized physical interfaces. However, at present, voice agents generally require wake-words and device names in order to specify the target of a spoken command (e.g., 'Hey Alexa, kitchen lights to full bright-ness'). In this research, we explore whether speech alone can be used as a directional communication channel, in much the same way visual gaze specifies a focus. Instead of a device's microphones simply receiving and processing spoken commands, we suggest they also infer the Direction of Voice (DoV). Our approach innately enables voice commands with addressability (i.e., devices know if a command was directed at them) in a natural and rapid manner. We quantify the accuracy of our implementation across users, rooms, spoken phrases, and other key factors that affect performance and usability. Taken together, we believe our DoV approach demonstrates feasibility and the promise of making distributed voice interactions much more intuitive and fluid.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1121–1131},
numpages = {11},
keywords = {voice interfaces, speaker orientation, addressability},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415818,
author = {Park, Keunwoo and Kim, Sunbum and Yoon, Youngwoo and Kim, Tae-Kyun and Lee, Geehyuk},
title = {DeepFisheye: Near-Surface Multi-Finger Tracking Technology Using Fisheye Camera},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415818},
doi = {10.1145/3379337.3415818},
abstract = {Near-surface multi-finger tracking (NMFT) technology expands the input space of touchscreens by enabling novel interactions such as mid-air and finger-aware interactions. We present DeepFisheye, a practical NMFT solution for mobile devices, that utilizes a fisheye camera attached at the bottom of a touchscreen. DeepFisheye acquires the image of an interacting hand positioned above the touchscreen using the camera and employs deep learning to estimate the 3D position of each fingertip. We created two new hand pose datasets comprising fisheye images, on which our network was trained. We evaluated DeepFisheye's performance for three device sizes. DeepFisheye showed average errors with approximate value of 20 mm for fingertip tracking across the different device sizes. Additionally, we created simple rule-based classifiers that estimate the contact finger and hand posture from DeepFisheye's output. The contact finger and hand posture classifiers showed accuracy of approximately 83 and 90%, respectively, across the device sizes.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1132–1146},
numpages = {15},
keywords = {near-surface, touchscreen, finger tracking, deep learning, computer vision},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415897,
author = {Wu, Erwin and Yuan, Ye and Yeo, Hui-Shyong and Quigley, Aaron and Koike, Hideki and Kitani, Kris M.},
title = {Back-Hand-Pose: 3D Hand Pose Estimation for a Wrist-Worn Camera via Dorsum Deformation Network},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415897},
doi = {10.1145/3379337.3415897},
abstract = {The automatic recognition of how people use their hands and fingers in natural settings -- without instrumenting the fingers -- can be useful for many mobile computing applications. To achieve such an interface, we propose a vision-based 3D hand pose estimation framework using a wrist-worn camera. The main challenge is the oblique angle of the wrist-worn camera, which makes the fingers scarcely visible. To address this, a special network that observes deformations on the back of the hand is required. We introduce DorsalNet, a two-stream convolutional neural network to regress finger joint angles from spatio-temporal features of the dorsal hand region (the movement of bones, muscle, and tendons). This work is the first vision-based real-time 3D hand pose estimator using visual features from the dorsal hand region. Our system achieves a mean joint-angle error of 8.81 degree for user-specific models and 9.77 degree for a general model. Further evaluation shows that our system outperforms previous work with an average of 20% higher accuracy in recognizing dynamic gestures, and achieves a 75% accuracy of detecting 11 different grasp types. We also demonstrate 3 applications which employ our system as a control device, an input device, and a grasped object recognizer.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1147–1160},
numpages = {14},
keywords = {wrist-worn devices, dorsal hand, 3d hand pose estimation},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415873,
author = {Takahashi, Ryo and Fukumoto, Masaaki and Han, Changyo and Sasatani, Takuya and Narusue, Yoshiaki and Kawahara, Yoshihiro},
title = {TelemetRing: A Batteryless and Wireless Ring-Shaped Keyboard Using Passive Inductive Telemetry},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415873},
doi = {10.1145/3379337.3415873},
abstract = {TelemetRing is a batteryless and wireless ring-shaped keyboard that supports command and text entry in daily lives by detecting finger typing on various surfaces. The proposed inductive telemetry approach eliminates bulky batteries or capacitors from the ring part. Each ring consists of a sensor coil (the ring part itself), 1-DoF piezoelectric accelerometer, and varactor diode; moreover, it has different resonant frequencies. Typing shocks slightly shift the resonant frequency, and these are detected by a wrist-mounted readout coil. 5-bit chord keyboard is realized by attaching five sensor rings on five fingers. Our evaluation shows that the prototype achieved the tiny (6 g, 3.5 cm^3) ring sensor and 89.7% of typing detection ratio.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1161–1168},
numpages = {8},
keywords = {wireless, ring, wearable, batteryless, coil, keyboard},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415848,
author = {Sarmah, Ritam Jyoti and Ding, Yunpeng and Wang, Di and Lee, Cheuk Yin Phipson and Li, Toby Jia-Jun and Chen, Xiang 'Anthony'},
title = {Geno: A Developer Tool for Authoring Multimodal Interaction on Existing Web Applications},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415848},
doi = {10.1145/3379337.3415848},
abstract = {Supporting voice commands in applications presents significant benefits to users. However, adding such support to existing GUI-based web apps is effort-consuming with a high learning barrier, as shown in our formative study, due to the lack of unified support for creating multi-modal interfaces. We develop Geno---a developer tool for adding the voice input modality to existing web apps without requiring significate NLP expertise. Geno provides a unified workflow for developers to specify functionalities to support by voice (intents), create language models for detecting intents and the relevant information (parameters) from user utterances, and fulfill the intents by either programmatically invoking the corresponding functions or replaying GUI actions on the web app. Geno further supports references to GUI context in voice commands (e.g., "add this to the playlist"). In a study, developers with little NLP expertise were able to add the multi-modal support for two existing web apps using Geno.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1169–1181},
numpages = {13},
keywords = {natural language processing, multimodal interaction, voice input, developer tool},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415839,
author = {Ball, Thomas and Kao, Shannon and Knoll, Richard and Zuniga, Daryl},
title = {TileCode: Creation of Video Games on Gaming Handhelds},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415839},
doi = {10.1145/3379337.3415839},
abstract = {We present TileCode, a video game creation environment that runs on battery-powered microcontroller-based gaming handhelds. Our work is motivated by the popularity of retro video games, the availability of low-cost gaming handhelds loaded with many such games, and the concomitant lack of a means to create games on the same handhelds. With TileCode, we seek to close the gap between the consumers and creators of video games and to motivate more individuals to participate in the design and creation of their own games. The TileCode programming model is based on tile maps and provides a visual means for specifying the context around a sprite, how a sprite should move based on that context, and what should happen upon sprite collisions. We demonstrate that a variety of popular video games can be programmed with TileCode using 10-15 visual rules and compare/contrast with block-based versions of the same games implemented using MakeCode Arcade.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1182–1193},
numpages = {12},
keywords = {cellular automata, gaming handhelds, video games, visual programming},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415872,
author = {Schoen, Andrew and Henrichs, Curt and Strohkirch, Mathias and Mutlu, Bilge},
title = {Authr: A Task Authoring Environment for Human-Robot Teams},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415872},
doi = {10.1145/3379337.3415872},
abstract = {Collaborative robots promise to transform work across many industries and promote human-robot teaming as a novel paradigm. However, realizing this promise requires the understanding of how existing tasks, developed for and performed by humans, can be effectively translated into tasks that robots can singularly or human-robot teams can collaboratively perform. In the interest of developing tools that facilitate this process we present Authr, an end-to-end task authoring environment that assists engineers at manufacturing facilities in translating existing manual tasks into plans applicable for human-robot teams and simulates these plans as they would be performed by the human and robot. We evaluated Authr with two user studies, which demonstrate the usability and effectiveness of Authr as an interface and the benefits of assistive task allocation methods for designing complex tasks for human-robot teams. We discuss the implications of these findings for the design of software tools for authoring human-robot collaborative plans.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1194–1208},
numpages = {15},
keywords = {task allocation, authoring, visual programming, human-robot collaboration},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415863,
author = {Takahashi, Haruki and Punpongsanon, Parinya and Kim, Jeeeun},
title = {Programmable Filament: Printed Filaments for Multi-Material 3D Printing},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415863},
doi = {10.1145/3379337.3415863},
abstract = {From full-color objects to functional capacitive artifacts, 3D printing multi-materials became essential to broaden the application areas of digital fabrication. We present Programmable Filament, a novel technique that enables multi-material printing using a commodity FDM 3D printer, requiring no hardware upgrades. Our technique builds upon an existing printing technique in which multiple filament segments are printed and spliced into a single threaded filament. We propose an end-to-end pipeline for 3D printing an object in multi-materials, with an introduction of the design systems for end-users. Optimized for low-cost, single-nozzle FDM 3D printers, the system is built upon our computational analysis and experiments to enhance its validity over various printers and materials to design and produce a programmable filament. Finally, we discuss application examples and speculate the future with its potential, such as custom filament manufacturing on-demand.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1209–1221},
numpages = {13},
keywords = {3d printing, fused deposition modeling, multiple materials, programmable matters},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415876,
author = {Forman, Jack and Dogan, Mustafa Doga and Forsythe, Hamilton and Ishii, Hiroshi},
title = {DefeXtiles: 3D Printing Quasi-Woven Fabric via Under-Extrusion},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415876},
doi = {10.1145/3379337.3415876},
abstract = {We present DefeXtiles, a rapid and low-cost technique to produce tulle-like fabrics on unmodified fused deposition modeling (FDM) printers. The under-extrusion of filament is a common cause of print failure, resulting in objects with periodic gap defects. In this paper, we demonstrate that these defects can be finely controlled to quickly print thinner, more flexible textiles than previous approaches allow. Our approach allows hierarchical control from micrometer structure to decameter form and is compatible with all common 3D printing materials. In this paper, we introduce the mechanism of DefeXtiles, establish the design space through a set of primitives with detailed workflows, and characterize the mechanical properties of DefeXtiles printed with multiple materials and parameters. Finally, we demonstrate the interactive features and new use cases of our approach through a variety of applications, such as fashion design prototyping, interactive objects, aesthetic patterning, and single-print actuators.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1222–1233},
numpages = {12},
keywords = {fabrics, personal fabrication, textiles, 3d printing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415590,
author = {Hofmann, Megan and Mankoff, Jennifer and Hudson, Scott E.},
title = {KnitGIST: A Programming Synthesis Toolkit for Generating Functional Machine-Knitting Textures},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415590},
doi = {10.1145/3379337.3415590},
abstract = {Automatic knitting machines are robust, digital fabrication devices that enable rapid and reliable production of attractive, functional objects by combining stitches to produce unique physical properties. However, no existing design tools support optimization for desirable physical and aesthetic knitted properties. We present KnitGIST (Generative Instantiation Synthesis Toolkit for knitting), a program synthesis pipeline and library for generating hand- and machine-knitting patterns by intuitively mapping objectives to tactics for texture design. KnitGIST generates a machine-knittable program in a domain-specific programming language.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1234–1247},
numpages = {14},
keywords = {generative design, program synthesis, knitting},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415893,
author = {Louis, Thibault and Troccaz, Jocelyne and Rochet-Capellan, Am\'{e}lie and B\'{e}rard, Fran\c{c}ois},
title = {GyroSuite: General-Purpose Interactions for Handheld Perspective Corrected Displays},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415893},
doi = {10.1145/3379337.3415893},
abstract = {Handheld Perspective-Corrected Displays (HPCDs) are physical objects that have a notable volume and that display a virtual 3D scene on their entire surface. Being handheld, they create the illusion of holding the scene in a physical container (the display). This has strong benefits for the intuitiveness of 3D interaction: manipulating objects of the virtual scene amounts to physical manipulations of the display. HPCDs have been limited so far to technical demonstrators and experimental tools to assess their merits. However, they show great potential as interactive systems for actual 3D applications. This requires that novel interactions be created to go beyond object manipulation and to offer general-purpose services such as menu command selection and continuous parameter control. Working with a two-handed spherical HPCD, we report on the design and informal evaluations of various interaction techniques for distant object selection, scene scaling, menu interaction and continuous parameter control. In particular, our design leverages the efficient two-handed control of the rotations of the display. We demonstrate how some of these techniques can be assemble in a self-contained anatomy learning application. Novice participants used the application in a qualitative user experiment. Most participants used the application effortlessly without any training or explanations.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1248–1260},
numpages = {13},
keywords = {gui, hpcd, spatial augmented reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3415812,
author = {Li, Nianlong and Kim, Han-Jong and Shen, LuYao and Tian, Feng and Han, Teng and Yang, Xing-Dong and Nam, Tek-Jin},
title = {HapLinkage: Prototyping Haptic Proxies for Virtual Hand Tools Using Linkage Mechanism},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415812},
doi = {10.1145/3379337.3415812},
abstract = {Haptic simulation of hand tools like wrenches, pliers, scissors and syringes are beneficial for finely detailed skill training in VR, but designing for numerous hand tools usually requires an expert-level knowledge of specific mechanism and protocol. This paper presents HapLinkage, a prototyping framework based on linkage mechanism, that provides typical motion templates and haptic renderers to facilitate proxy design of virtual hand tools. The mechanical structures can be easily modified, for example, to scale the size, or to change the range of motion by selectively changing linkage lengths. Resistant, stop, release, and restoration force feedback are generated by an actuating module as part of the structure. Additional vibration feedback can be generated with a linear actuator. HapLinkage enables easy and quick prototypting of hand tools for diverse VR scenarios, that embody both of their kinetic and haptic properties. Based on interviews with expert designers, it was confirmed that HapLinkage is expressive in designing haptic proxy of hand tools to enhance VR experiences. It also identified potentials and future development of the framework.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1261–1274},
numpages = {14},
keywords = {linkage mechanism, prototyping haptic interface, virtual reality, haptic proxies, virtual hand tool},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3379337.3422379,
author = {Costanza-Chock, Sasha},
title = {Design Justice and User Interface Design},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3422379},
doi = {10.1145/3379337.3422379},
abstract = {In this keynote talk, Dr. Costanza-Chock will explore the theory and practice of design justice, discuss how design affordances, disaffordances, and dysaffordances distribute benefits and burdens unequally according to users? location within the matrix of domination (white supremacy, heteropatriarchy, ableism, capitalism, and settler colonialism), and invite us to consider how user interface designers can intentionally contribute to building ?a better world?, a world where many worlds fit; linked worlds of collective liberation and ecological sustainability.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {1275},
numpages = {1},
keywords = {dysaffordances, matrix of domination, disaffordances, design justice, design, affordances},
location = {Virtual Event, USA},
series = {UIST '20}
}

