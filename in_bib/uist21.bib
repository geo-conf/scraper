@inproceedings{10.1145/3472749.3474731,
author = {Conlen, Matthew and Vo, Megan and Tan, Alan and Heer, Jeffrey},
title = {Idyll Studio: A Structured Editor for Authoring Interactive &amp; Data-Driven Articles},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474731},
doi = {10.1145/3472749.3474731},
abstract = {Interactive articles are an effective medium of communication in education, journalism, and scientific publishing, yet are created using complex general-purpose programming tools. We present Idyll Studio, a structured editor for authoring and publishing interactive and data-driven articles. We extend the Idyll framework to support reflective documents, which can inspect and modify their underlying program at runtime, and show how this functionality can be used to reify the constituent parts of a reactive document model—components, text, state, and styles—in an expressive, interoperable, and easy-to-learn graphical interface. In a study with 18 diverse participants, all could perform basic editing and composition, use datasets and variables, and specify relationships between components. Most could choreograph interactive visualizations and dynamic text, although some struggled with advanced uses requiring unstructured code editing. Our findings suggest Idyll Studio lowers the threshold for non-experts to create interactive articles and allows experts to rapidly specify a wide range of article designs. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {1–12},
numpages = {12},
keywords = {explorable explanations, computational media, interactive articles},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474732,
author = {Jingu, Arata and Kamigaki, Takaaki and Fujiwara, Masahiro and Makino, Yasutoshi and Shinoda, Hiroyuki},
title = {LipNotif: Use of Lips as a Non-Contact Tactile Notification Interface Based on Ultrasonic Tactile Presentation},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474732},
doi = {10.1145/3472749.3474732},
abstract = { We propose LipNotif, a non-contact tactile notification system that uses airborne ultrasound tactile presentation to lips. Lips are suitable for non-contact tactile notifications because they have high tactile sensitivity comparable to the palms, are less occupied in daily life, and are constantly exposed outward. LipNotif uses tactile patterns to intuitively convey information to users, allowing them to receive notifications using only their lips, without sight, hearing, or hands. We developed a prototype system that automatically recognizes the position of the lips and presents non-contact tactile sensations. Two experiments were conducted to evaluate the feasibility of LipNotif. In the first experiment, we found that directional information can be notified to the lips with an average accuracy of ± 11.1° in the 120° horizontal range. In the second experiment, we could elicit significantly different affective responses by changing the stimulus intensity. The experimental results indicated that LipNotif is practical for conveying directions, emotions, and combinations of them. LipNotif can be applied for various purposes, such as notifications during work, calling in the waiting room, and tactile feedback in automotive user interfaces.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {13–23},
numpages = {11},
keywords = {notification, lip, mid-air haptics},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474733,
author = {Dogan, Mustafa Doga and Acevedo Colon, Steven Vidal and Sinha, Varnika and Ak\c{s}it, Kaan and Mueller, Stefanie},
title = {SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474733},
doi = {10.1145/3472749.3474733},
abstract = {Laser cutter users face difficulties distinguishing between visually similar materials. This can lead to problems, such as using the wrong power/speed settings or accidentally cutting hazardous materials. To support users, we present SensiCut, an integrated material sensing platform for laser cutters. SensiCut enables material awareness beyond what users are able to see and reliably differentiates among similar-looking types. It achieves this by detecting materials’ surface structures using speckle sensing and deep learning. SensiCut consists of a compact hardware add-on for laser cutters and a user interface that integrates material sensing into the laser cutting workflow. In addition to improving the traditional workflow and its safety1, SensiCut enables new applications, such as automatically partitioning designs when engraving on multi-material objects or adjusting their geometry based on the kerf of the identified material. We evaluate SensiCut’s accuracy for different types of materials under different sheet orientations and illumination conditions. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {24–38},
numpages = {15},
keywords = {speckle sensing., laser cutter support tools, material identification, personal fabrication},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474734,
author = {Fran\c{c}oise, Jules and Caramiaux, Baptiste and Sanchez, T\'{e}o},
title = {Marcelle: Composing Interactive Machine Learning Workflows and Interfaces},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474734},
doi = {10.1145/3472749.3474734},
abstract = {Human-centered approaches to machine learning have established theoretical foundations, design principles and interaction techniques to facilitate end-user interaction with machine learning systems. Yet, general-purpose toolkits supporting the design of interactive machine learning systems are still missing, despite their potential to foster reuse, appropriation and collaboration between different stakeholders including developers, machine learning experts, designers and end users. In this paper, we present an architectural model for toolkits dedicated to the design of human interactions with machine learning. The architecture is built upon a modular collection of interactive components that can be composed to build interactive machine learning workflows, using reactive pipelines and composable user interfaces. We introduce Marcelle, a toolkit for the design of human interactions with machine learning that implements this model. We illustrate Marcelle with two implemented case studies: (1) a HCI researcher conducts user studies to understand novice interaction with machine learning, and (2) a machine learning expert and a clinician collaborate to develop a skin cancer diagnosis system. Finally, we discuss our experience with the toolkit, along with its limitation and perspectives. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {39–53},
numpages = {15},
keywords = {Interactive Machine Learning, Architectural Model, Machine Teaching, Toolkit.},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474735,
author = {Lafreniere, Ben and R. Jonker, Tanya and Santosa, Stephanie and Parent, Mark and Glueck, Michael and Grossman, Tovi and Benko, Hrvoje and Wigdor, Daniel},
title = {False Positives vs. False Negatives: The Effects of Recovery Time and Cognitive Costs on Input Error Preference},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474735},
doi = {10.1145/3472749.3474735},
abstract = {Existing approaches to trading off false positive versus false negative errors in input recognition are based on imprecise ideas of how these errors affect user experience that are unlikely to hold for all situations. To inform dynamic approaches to setting such a tradeoff, two user studies were conducted on how relative preference for false positive versus false negative errors is influenced by differences in the temporal cost of error recovery, and high-level task factors (time pressure, multi-tasking). Participants completed a tile selection task in which false positive and false negative errors were injected at a fixed rate, and the temporal cost to recover from each of the two types of error was varied, and then indicated a preference for one error type or the other, and a frustration rating for the task. Responses indicate that the temporal costs of error recovery can drive both frustration and relative error type preference, and that participants exhibit a bias against false positive errors, equivalent to ∼1.5 seconds or more of added temporal recovery time. Several explanations for this bias were revealed, including that false positive errors impose a greater attentional demand on the user, and that recovering from false positive errors imposes a task switching cost.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {54–68},
numpages = {15},
keywords = {gesture recognition, recognizer thresholds, input ambiguity, utility models, Error perception, probabilistic input},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474736,
author = {Brackenbury, Will and McNutt, Andrew and Chard, Kyle and Elmore, Aaron and Ur, Blase},
title = {KondoCloud: Improving Information Management in Cloud Storage via Recommendations Based on File Similarity},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474736},
doi = {10.1145/3472749.3474736},
abstract = {Users face many challenges in keeping their personal file collections organized. While current file-management interfaces help users retrieve files in disorganized repositories, they do not aid in organization. Pertinent files can be difficult to find, and files that should have been deleted may remain. To help, we designed KondoCloud, a file-browser interface for personal cloud storage. KondoCloud makes machine learning-based recommendations of files users may want to retrieve, move, or delete. These recommendations leverage the intuition that similar files should be managed similarly. We developed and evaluated KondoCloud through two complementary online user studies. In our Observation Study, we logged the actions of 69 participants who spent 30 minutes manually organizing their own Google Drive repositories. We identified high-level organizational strategies, including moving related files to newly created sub-folders and extensively deleting files. To train the classifiers that underpin KondoCloud’s recommendations, we had participants label whether pairs of files were similar and whether they should be managed similarly. In addition, we extracted ten metadata and content features from all files in participants’ repositories. Our logistic regression classifiers all achieved F1 scores of 0.72 or higher. In our Evaluation Study, 62 participants used KondoCloud either with or without recommendations. Roughly half of participants accepted a non-trivial fraction of recommendations, and some participants accepted nearly all of them. Participants who were shown the recommendations were more likely to delete related files located in different directories. They also generally felt the recommendations improved efficiency. Participants who were not shown recommendations nonetheless manually performed about a third of the actions that would have been recommended. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {69–83},
numpages = {15},
keywords = {cloud storage, Google Drive, recommendations, personal information management},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474737,
author = {Naik, Aaditya and Mendelson, Jonathan and Sands, Nathaniel and Wang, Yuepeng and Naik, Mayur and Raghothaman, Mukund},
title = {Sporq: An Interactive Environment for Exploring Code Using Query-by-Example},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474737},
doi = {10.1145/3472749.3474737},
abstract = { There has been widespread adoption of IDEs and powerful tools for program analysis. However, programmers still find it difficult to conveniently analyze their code for custom patterns. Such systems either provide inflexible interfaces or require knowledge of complex query languages and compiler internals. In this paper, we present Sporq, a tool that allows developers to mine their codebases for a range of patterns, including bugs, code smells, and violations of coding standards. Sporq offers an interactive environment in which the user highlights program elements, and the system responds by identifying other parts of the codebase with similar patterns. The programmer can then provide feedback which enables the system to rapidly infer the programmer’s intent. Internally, our system is driven by high-fidelity relational program representations and algorithms to synthesize database queries from examples. Our experiments and user studies with a VS Code extension indicate that Sporq reduces the effort needed by programmers to write custom analyses and discover bugs in large codebases.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {84–99},
numpages = {16},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474738,
author = {Endo, Isamu and Takashima, Kazuki and Inoue, Maakito and Fujita, Kazuyuki and Kiyokawa, Kiyoshi and Kitamura, Yoshifumi},
title = {ModularHMD: A Reconfigurable Mobile Head-Mounted Display Enabling Ad-Hoc Peripheral Interactions with the Real World},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474738},
doi = {10.1145/3472749.3474738},
abstract = { We propose ModularHMD, a new mobile head-mounted display concept, which adopts a modular mechanism and allows a user to perform ad-hoc peripheral interaction with real-world devices or people during VR experiences. ModularHMD is comprised of a central HMD and three removable module devices installed in the periphery of the HMD cowl. Each module has four main states: occluding, extended VR view, video see-through (VST), and removed/reused. Among different combinations of module states, a user can quickly setup the necessary HMD forms, functions, and real-world visions for ad-hoc peripheral interactions without removing the headset. For instance, an HMD user can see her surroundings by switching a module into the VST mode. She can also physically remove a module to obtain direct peripheral visions of the real world. The removed module can be reused as an instant interaction device (e.g., touch keyboards) for subsequent peripheral interactions. Users can end the peripheral interaction and revert to a full VR experience by re-mounting the module. We design ModularHMD’s configuration and peripheral interactions with real-world objects and people. We also implement a proof-of-concept prototype of ModularHMD to validate its interactions capabilities through a user study. Results show that ModularHMD is an effective solution that enables both immersive VR and ad-hoc peripheral interactions.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {100–117},
numpages = {18},
keywords = {shape-changing interface, situational awareness, Asymmetric interaction},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474739,
author = {Kari, Mohamed and Grosse-Puppendahl, Tobias and Jagaciak, Alexander and Bethge, David and Sch\"{u}tte, Reinhard and Holz, Christian},
title = {SoundsRide: Affordance-Synchronized Music Mixing for In-Car Audio Augmented Reality},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474739},
doi = {10.1145/3472749.3474739},
abstract = {Music is a central instrument in video gaming to attune a player’s attention to the current atmosphere and increase their immersion in the game. We transfer the idea of scene-adaptive music to car drives and propose SoundsRide, an in-car audio augmented reality system that mixes music in real-time synchronized with sound affordances along the ride. After exploring the design space of affordance-synchronized music, we design SoundsRide to temporally and spatially align high-contrast events on the route, e.&nbsp;g., highway entrances or tunnel exits, with high-contrast events in music, e.&nbsp;g., song transitions or beat drops, for any recorded and annotated GPS trajectory by a three-step procedure. In real-time, SoundsRide 1) estimates temporal distances to events on the route, 2) fuses these novel estimates with previous estimates in a cost-aware music-mixing plan, and 3) if necessary, re-computes an updated mix to be propagated to the audio output. To minimize user-noticeable updates to the mix, SoundsRide fuses new distance information with a filtering procedure that chooses the best updating strategy given the last music-mixing plan, the novel distance estimations, and the system parameterization. We technically evaluate SoundsRide and conduct a user evaluation with 8 participants to gain insights into how users perceive SoundsRide in terms of mixing, affordances, and synchronicity. We find that SoundsRide can create captivating music experiences and positively as well as negatively influence subjectively perceived driving safety, depending on the mix and user. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {118–133},
numpages = {16},
keywords = {auditory augmented reality, sound affordances, context-adaptive music},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474740,
author = {Hu, Jingmei and Vaithilingam, Priyan and Chong, Stephen and Seltzer, Margo and Glassman, Elena L.},
title = {Assuage: Assembly Synthesis Using A Guided Exploration},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474740},
doi = {10.1145/3472749.3474740},
abstract = {Assembly programming is challenging, even for experts. Program synthesis, as an alternative to manual implementation, has the potential to enable both expert and non-expert users to generate programs in an automated fashion. However, current tools and techniques are unable to synthesize assembly programs larger than a few instructions. We present Assuage : ASsembly Synthesis Using A Guided Exploration, which is a parallel interactive assembly synthesizer that engages the user as an active collaborator, enabling synthesis to scale beyond current limits. Using Assuage, users can provide two types of semantically meaningful hints that expedite synthesis and allow for exploration of multiple possibilities simultaneously. Assuage exposes information about the underlying synthesis process using multiple representations to help users guide synthesis. We conducted a within-subjects study with twenty-one participants working on assembly programming tasks. With Assuage, participants with a wide range of expertise were able to achieve significantly higher success rates, perceived less subjective workload, and preferred the usefulness and usability of Assuage over a state of the art synthesis tool. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {134–148},
numpages = {15},
keywords = {assembly programming, Program synthesis, interactive synthesis},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474741,
author = {Pfeuffer, Ken and Dinc, Abdullatif and Obernolte, Jan and Rivu, Radiah and Abdrabou, Yasmeen and Shelter, Franziska and Abdelrahman, Yomna and Alt, Florian},
title = {Bi-3D: Bi-Manual Pen-and-Touch Interaction for 3D Manipulation on Tablets},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474741},
doi = {10.1145/3472749.3474741},
abstract = { Tablets are attractive for design work anywhere, but 3D manipulations are notoriously difficult. We explore how engaging the stylus and multi-touch in concert can render such tasks easier. We introduce Bi-3D, an interaction concept where touch gestures are combined with 2D pen commands for 3D manipulation. For example, for a fast and intuitive 3D drag &amp; drop technique: the pen drags the object on-screen, and parallel pinch-to-zoom moves it in the third dimension. In this paper, we describe the Bi-3D design space, crossing two-handed input and the degrees-of-freedom (DOF) of 3D manipulation and navigation tasks. We demonstrate sketching and manipulation tools in a prototype 3D design application, where users can fluidly combine 3D operations through alternating and parallel use of the modalities. We evaluate the core technique, bi-manual 3DOF input, against widget and mid-air baselines in an object movement task. We find that Bi-3D is a fast and practical way for multi-dimensional manipulation of graphical objects, promising to facilitate 3D design on stylus and tablet devices.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {149–161},
numpages = {13},
keywords = {3D design, bi-manual interface, pen, tablet computing, multi-touch},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474742,
author = {Zhao, Maozheng and Cui, Wenzhe and Ramakrishnan, IV and Zhai, Shumin and Bi, Xiaojun},
title = {Voice and Touch Based Error-Tolerant Multimodal Text Editing and Correction for Smartphones},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474742},
doi = {10.1145/3472749.3474742},
abstract = { Editing operations such as cut, copy, paste, and correcting errors in typed text are often tedious and challenging to perform on smartphones. In this paper, we present VT, a voice and touch-based multi-modal text editing and correction method for smartphones. To edit text with VT, the user glides over a text fragment with a finger and dictates a command, such as ”bold” to change the format of the fragment, or the user can tap inside a text area and speak a command such as ”highlight this paragraph” to edit the text. For text correcting, the user taps approximately at the area of erroneous text fragment and dictates the new content for substitution or insertion. VT combines touch and voice inputs with language context such as language model and phrase similarity to infer a user’s editing intention, which can handle ambiguities and noisy input signals. It is a great advantage over the existing error correction methods (e.g., iOS’s Voice Control) which require precise cursor control or text selection. Our evaluation shows that VT significantly improves the efficiency of text editing and text correcting on smartphones over the touch-only method and the iOS’s Voice Control method. Our user studies showed that VT reduced the text editing time by 30.80%, and text correcting time by 29.97% over the touch-only method. VT reduced the text editing time by 30.81%, and text correcting time by 47.96% over the iOS’s Voice Control method. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {162–178},
numpages = {17},
keywords = {Multimodal interaction, touch input, smartphones., text editing, text correction},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474743,
author = {Wang, Liang and Huang, Zhe and Zhou, Ziyu and McKeon, Devon and Blaney, Giles and Hughes, Michael C. and Jacob, Robert J. K.},
title = {Taming FNIRS-Based BCI Input for Better Calibration and Broader Use},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474743},
doi = {10.1145/3472749.3474743},
abstract = {Brain-computer interfaces (BCI) are an emerging technology with many potential applications. Functional near-infrared spectroscopy (fNIRS) can provide a convenient and unobtrusive real time input for BCI. fNIRS is especially promising as a signal that could be used to automatically classify a user’s current cognitive workload. However, the data needed to train such a classifier is currently not widely available, difficult to collect, and difficult to interpret due to noise and cross-subject variation. A further challenge is the need for significant user-specific calibration. To address these issues, we introduce a new dataset gathered from 15 subjects and a new multi-stage supervised machine learning pipeline. Our approach learns from both observed data and augmented data derived from multiple subjects in its early stages, and then fine-tunes predictions to an individual subject in its last stage. We show promising gains in accuracy in a standard “n-back” cognitive workload classification task compared to baselines that use only subject-specific data or only group-level data, even when our approach is given much less subject-specific data. Even though these experiments analyzed the data retrospectively, we carefully removed anything from our process that could not have been done in real time, because our process is targeted at future real-time operation. This paper contributes a new dataset, a new multi-stage training pipeline, results showing significant improvement compared to alternative pipelines, and discussion of the implications for user interface design. Our complete dataset and software are publicly available at https://tufts-hci-lab.github.io/code_and_datasets/. We hope these results make fNIRS-based interactive brain input easier for a wide range of future researchers and designers to explore.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {179–197},
numpages = {19},
keywords = {Brain-Computer Interface, cognitive workload, neural networks, n-back task, near-infrared spectroscopy, implicit interfaces, BCI, machine learning, data augmentation, fNIRS},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474744,
author = {Shrestha, Nischal and Barik, Titus and Parnin, Chris},
title = {Unravel: A Fluent Code Explorer for Data Wrangling},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474744},
doi = {10.1145/3472749.3474744},
abstract = { Data scientists have adopted a popular design pattern in programming called the fluent interface for composing data wrangling code. The fluent interface works by combining multiple transformations on a data table—or dataframes—with a single chain of expressions, which produces an output. Although fluent code promotes legibility, the intermediate dataframes are lost, forcing data scientists to unravel the chain through tedious code edits and re-execution. Existing tools for data scientists do not allow easy exploration or support understanding of fluent code. To address this gap, we designed a tool called Unravel that enables structural edits via drag-and-drop and toggle switch interactions to help data scientists explore and understand fluent code. Data scientists can apply simple structural edits via drag-and-drop and toggle switch interactions to reorder and (un)comment lines. To help data scientists understand fluent code, Unravel provides function summaries and always-on visualizations highlighting important changes to a dataframe. We discuss the design motivations behind Unravel and how it helps understand and explore fluent code. In a first-use study with 14 data scientists, we found that Unravel facilitated diverse activities such as validating assumptions about the code or data, exploring alternatives, and revealing function behavior. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {198–207},
numpages = {10},
keywords = {data science, programming, data wrangling},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474745,
author = {Oh, Seungjae and Park, Chaeyong and Jeon, Yo-Seb and Choi, Seungmoon},
title = {Identifying Contact Fingers on Touch Sensitive Surfaces by Ring-Based Vibratory Communication},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474745},
doi = {10.1145/3472749.3474745},
abstract = {As computing paradigms shift toward mobile and ubiquitous interaction, there is an increasing demand for wearable interfaces supporting multifaceted input in smart living environments. In this regard, we introduce a system that identifies contact fingers using vibration as a modality of communication. We investigate the vibration characteristics of the communication channels involved and simulate the transmission of vibration sequences. In the simulation, we test and refine modulation and demodulation methods to design vibratory communication protocols that are robust to environmental noises and can detect multiple simultaneous contact fingers. As a result, we encode an on-off keying sequence with a unique carrier frequency to each finger and demodulate the sequences by applying cross-correlation. We verify the communication protocols in two environments, laboratory and cafe, where the resulting highest accuracy was 93&nbsp;% and 90.5&nbsp;%, respectively. Our system achieves over 91&nbsp;% accuracy in identifying seven contact states from three fingers while wearing only two actuator rings with the aid of a touch screen. Our findings shed light on diversifying touch interactions on rigid surfaces by means of vibratory communication. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {208–222},
numpages = {15},
keywords = {Finger Identification, Vibratory Communication, Vibration Sensing, Touch Interaction},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474746,
author = {Li, Jiannan and Lyu, Jiahe and Sousa, Mauricio and Balakrishnan, Ravin and Tang, Anthony and Grossman, Tovi},
title = {Route Tapestries: Navigating 360° Virtual Tour Videos Using Slit-Scan Visualizations},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474746},
doi = {10.1145/3472749.3474746},
abstract = { An increasingly popular way of experiencing remote places is by viewing 360° virtual tour videos, which show the surrounding view while traveling through an environment. However, finding particular locations in these videos can be difficult because current interfaces rely on distorted frame previews for navigation. To alleviate this usability issue, we propose Route Tapestries, continuous orthographic-perspective projection of scenes along camera routes. We first introduce an algorithm for automatically constructing Route Tapestries from a 360° video, inspired by the slit-scan photography technique. We then present a desktop video player interface using a Route Tapestry timeline for navigation. An online evaluation using a target-seeking task showed that Route Tapestries allowed users to locate targets 22% faster than with YouTube-style equirectangular previews and reduced the failure rate by 75% compared to a more conventional row-of-thumbnail strip preview. Our results highlight the value of reducing visual distortion and providing continuous visual contexts in previews for navigating 360°virtual tour videos.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {223–238},
numpages = {16},
keywords = {Navigation, Virtual Tour, 360° Video},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474747,
author = {Lu, Jasmine and Liu, Ziwei and Brooks, Jas and Lopes, Pedro},
title = {Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474747},
doi = {10.1145/3472749.3474747},
abstract = {We propose a new class of haptic devices that provide haptic sensations by delivering liquid-stimulants to the user's skin; we call this chemical haptics. Upon absorbing these stimulants, which contain safe and small doses of key active ingredients, receptors in the user's skin are chemically triggered, rendering distinct haptic sensations. We identified five chemicals that can render lasting haptic sensations: tingling (sanshool), numbing (lidocaine), stinging (cinnamaldehyde), warming (capsaicin), and cooling (menthol). To enable the application of our novel approach in a variety of settings (such as VR), we engineered a self-contained wearable that can be worn anywhere on the user's skin (e.g., face, arms, legs). Implemented as a soft silicone patch, our device uses micropumps to push the liquid stimulants through channels that are open to the user's skin, enabling topical stimulants to be absorbed by the skin as they pass through. Our approach presents two unique benefits. First, it enables sensations, such as numbing, not possible with existing haptic devices. Second, our approach offers a new pathway, via the skin's chemical receptors, for achieving multiple haptic sensations using a single actuator, which would otherwise require combining multiple actuators (e.g., Peltier, vibration motors, electro-tactile stimulation). We evaluated our approach by means of two studies. In our first study, we characterized the temporal profiles of sensations elicited by each chemical. Using these insights, we designed five interactive VR experiences utilizing chemical haptics, and in our second user study, participants rated these VR experiences with chemical haptics as more immersive than without. Finally, as the first work exploring the use of chemical haptics on the skin, we offer recommendations to designers for how they may employ our approach for their interactive experiences.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {239–257},
numpages = {19},
keywords = {virtual reality, chemicals, fluidics, haptic feedback, tactile haptics},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474748,
author = {Ni, Wode and Sunshine, Joshua and Le, Vu and Gulwani, Sumit and Barik, Titus},
title = {ReCode : A Lightweight Find-and-Replace Interaction in the IDE for Transforming Code by Example},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474748},
doi = {10.1145/3472749.3474748},
abstract = { Software developers frequently confront a recurring challenge of making code transformations—similar but not entirely identical code changes in many places—in their integrated development environments. Through formative interviews (n = 7), we found that developers were aware of many tools intended to help with code transformations, but often made their changes manually because these tools required too much expertise or effort to be able to use effectively. To address these needs, we built an extension for Visual Studio Code, called reCode. reCode improves the familiar find-and-replace experience by allowing the developer to specify a straightforward search term to identify relevant locations, and then demonstrate their intended changes by simply typing a change directly in the editor. Using programming by example, reCode automatically learns a more general code transformation and displays these transformations as before-and-after differences inline, with clickable actions to interactively accept, reject, or refine the proposed changes. In our usability evaluation (n = 12), developers reported that this mixed-initiative, example-driven experience is intuitive, complements their existing workflow, and offers a unified approach to conveniently tackle a variety of common yet frustrating scenarios for code transformations.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {258–269},
numpages = {12},
keywords = {find-and-replace, program synthesis, code transformation},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474749,
author = {Tian, Rundong and Paulos, Eric},
title = {Adroid: Augmenting Hands-on Making with a Collaborative Robot},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474749},
doi = {10.1145/3472749.3474749},
abstract = {Adroid1 enables users to borrow precision and accuracy from a robotic arm when using hand-held tools. When a tool is mounted to the robot, the user can hold and move the tool directly—Adroid measures the user’s applied forces and commands the robot to move in response. Depending on the tool and scenario, Adroid can selectively restrict certain motions. In the resulting interaction, the robot acts like a virtual “jig” which constrains the tool’s motion, augmenting the user’s accuracy, technique, and strength, while not diminishing their agency during open-ended fabrication tasks. We complement these hands-on interactions with projected augmented reality for visual feedback about the state of the system. We show how tools augmented by Adroid can support hands-on making and discuss how it can be configured to support other tasks within and beyond fabrication. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {270–281},
numpages = {12},
keywords = {Lucid Fabrication, Digital Fabrication, Interactive Fabrication, Haptics, Robots, Projection AR},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474750,
author = {Cheng, Yifei and Yan, Yukang and Yi, Xin and Shi, Yuanchun and Lindlbauer, David},
title = {SemanticAdapt: Optimization-Based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474750},
doi = {10.1145/3472749.3474750},
abstract = {We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated for MR systems to be beneficial for end users. We contribute an approach that formulates this challenge as a combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. To achieve this, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous layouts. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic associations into account, our approach decreased the number of manual interface adaptations by 33%. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {282–297},
numpages = {16},
keywords = {Mixed Reality, Adaptive user interfaces, Computational interaction},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474751,
author = {Yan, Zeyu and Peng, Huaishu},
title = {FabHydro: Printing Interactive Hydraulic Devices with an Affordable SLA 3D Printer},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474751},
doi = {10.1145/3472749.3474751},
abstract = { We introduce FabHydro, a set of rapid and low-cost methods to prototype interactive hydraulic devices based on an off-the-shelf 3D printer and flexible photosensitive resin. We first present printer settings and custom support structures to warrant the successful print of flexible and deformable objects. We then demonstrate two printing methods to seal the transmission fluid inside these deformable structures: the Submerged Printing process that seals the liquid resin without manual assembly, and the Printing with Plugs method that allows the use of different transmission fluids without modification to the printer. Following the printing methods, we report a design space with a range of 3D printable primitives, including the hydraulic generator, transmitter, and actuator. To demonstrate the feasibility of our approaches and the breadth of new designs that they enable, we showcase a set of examples from a printed robotic gripper that can be operated at a distance to a mobile phone stand that serves as a status reminder by repositioning the user’s phone. We conclude with a discussion of our approach’s limitations and possible future improvements.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {298–311},
numpages = {14},
keywords = {Fabrication, Design, Interaction, 3D Printing},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474752,
author = {Riva, Oriana and Kace, Jason},
title = {Etna: Harvesting Action Graphs from Websites},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474752},
doi = {10.1145/3472749.3474752},
abstract = { Knowledge bases, such as Google knowledge graph, contain millions of entities (people, places, etc.) and billions of facts about them. While much is known about entities, little is known about the actions these entities relate to. On the other hand, the Web has lots of information about human tasks. A website for restaurant reservations, for example, implicitly knows about various restaurant-related actions (making reservations, delivering food, etc.), the inputs these actions require and their expected output; it can also be automated to execute those actions. To harvest action knowledge from websites, we propose Etna. Users demonstrate how to accomplish various tasks in a website, and Etna constructs an action-state model of the website visualized as an action graph. An action graph includes definitions of tasks and actions, knowledge about their start/end states, and execution scripts for their automation. We report on our experience in building action-state models of many commercial websites and use cases that leveraged them.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {312–331},
numpages = {20},
keywords = {Action graphs, UI automation., graphical user interfaces, programming by demonstration, web},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474753,
author = {Gamage, Nisal Menuka and Ishtaweera, Deepana and Weigel, Martin and Withana, Anusha},
title = {So Predictable! Continuous 3D Hand Trajectory Prediction in Virtual Reality},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474753},
doi = {10.1145/3472749.3474753},
abstract = {We contribute a novel user- and activity-independent kinematics-based regressive model for continuously predicting ballistic hand movements in virtual reality (VR). Compared to prior work on end-point prediction, continuous hand trajectory prediction in VR enables an early estimation of future events such as collisions between the user’s hand and virtual objects such as UI widgets. We developed and validated our prediction model through a user study with 20 participants. The study collected hand motion data with a 3D pointing task and a gaming task with three popular VR games. Results show that our model can achieve a low Root Mean Square Error (RMSE) of 0.80&nbsp;cm, 0.85&nbsp;cm and 3.15&nbsp;cm from future hand positions ahead of 100&nbsp;ms, 200&nbsp;ms and 300&nbsp;ms respectively across all the users and activities. In pointing tasks, our predictive model achieves an average angular error of 4.0° and 1.5° from the true landing position when 50% and 70% of the way through the movement. A follow-up study showed that the model can be applied to new users and new activities without further training. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {332–343},
numpages = {12},
keywords = {Virtual Reality, Hand motion prediction, Activity-independent, User-independent, Kinematics-based Model},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474754,
author = {Chang, Ruei-Che and Tsao, Chih-An and Liao, Fang-Ying and Yong, Seraphina and Yeh, Tom and Chen, Bing-Yu},
title = {Daedalus in the Dark: Designing for Non-Visual Accessible Construction of Laser-Cut Architecture},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474754},
doi = {10.1145/3472749.3474754},
abstract = { Design tools and research regarding laser-cut architectures have been widely explored in the past decade. However, such discussion has mostly revolved around technical and structural design questions instead of another essential element of laser-cut models — assembly — a process that relies heavily on components’ visual affordance, therefore less accessible to blind or low vision (BLV) people. To narrow the gap in this area, we co-designed with 7 BLV people to examine their assembly experience with different laser-cut architectures. From their feedback, we proposed several design heuristics and guidelines for Daedalus, a generative design tool that can produce tactile aids for laser-cut assembly given a few high-level manual inputs. We validate the proposed aids in a user study with 8 new BLV participants. Our results revealed that BLV users can manage laser-cut assembly more efficiently with Daedalus. Going forth from this design iteration, we discuss implications for future research on accessible laser-cut assembly.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {344–358},
numpages = {15},
keywords = {Fabrication, Accessibility, Laser Cutting, Assistive Technology, User-centered Design, Assembly, Prototyping},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474755,
author = {L., Jane and Zhai, Kevin Y. and Echevarria, Jose and Fried, Ohad and Hanrahan, Pat and Landay, James A.},
title = {Dynamic Guidance for Decluttering Photographic Compositions},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474755},
doi = {10.1145/3472749.3474755},
abstract = { Unwanted clutter in a photo can be incredibly distracting. However in the moment, photographers have so many things to simultaneously consider, it can be hard to catch every detail. Designers have long known the benefits of abstraction for seeing a more holistic view of their design. We wondered if, similarly, some form of image abstraction might be helpful for photographers as an alternative perspective or “lens” with which to see their image. Specifically, we wondered if such abstraction might draw the photographer’s attention away from details in the subject to noticing objects in the background, such as unwanted clutter. We present our process for designing such a camera overlay, based on the idea of using abstraction to recognize clutter. Our final design uses object-based saliency and edge detection to highlight contrast along subject and image borders, outlining potential distractors in these regions. We describe the implementation and evaluation of a capture-time tool that interactively displays these overlays and find that the tool is helpful for making users more confident in their ability to take decluttered photos that clearly convey their intended story.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {359–371},
numpages = {13},
keywords = {declutter, photography, camera interfaces, composition},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474756,
author = {Yu, Xue and DiVerdi, Stephen and Sharma, Akshay and Gingold, Yotam},
title = {ScaffoldSketch: Accurate Industrial Design Drawing in VR},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474756},
doi = {10.1145/3472749.3474756},
abstract = {We present an approach to in-air design drawing based on the two-stage approach common in 2D design drawing practice. The primary challenge to 3D drawing in-air is the accuracy of users’ strokes. Beautifying or auto-correcting an arbitrary drawing in 2D or 3D is challenging due to ambiguities stemming from many possible interpretations of a stroke. A similar challenge appears when drawing freehand on paper in the real world. 2D design drawing practice (as taught in industrial design school) addresses this by decomposing the process of creating realistic 2D projections of 3D shapes. Designers first create scaffold or construction lines. When drawing shape or structure curves, designers are guided by the scaffolds. Our key insight is that accurate industrial design drawing in 3D becomes tractable when decomposed into auto-correcting scaffold strokes, which have simple relationships with one another, followed by auto-correcting shape strokes with respect to the scaffold strokes. We demonstrate our approach’s effectiveness with an expert study involving industrial designers. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {372–384},
numpages = {13},
keywords = {virtual reality, sketching, auto-correct, industrial design},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474757,
author = {Petridis, Savvas and Shin, Hijung Valentina and Chilton, Lydia B},
title = {SymbolFinder: Brainstorming Diverse Symbols Using Local Semantic Networks},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474757},
doi = {10.1145/3472749.3474757},
abstract = { Visual symbols are the building blocks for visual communication. They convey abstract concepts like reform and participation quickly and effectively. When creating graphics with symbols, novice designers often struggle to brainstorm multiple, diverse symbols because they fixate on a few associations instead of broadly exploring different aspects of the concept. We present SymbolFinder, an interactive tool for finding visual symbols for abstract concepts. SymbolFinder molds symbol-finding into a recognition rather than recall task by introducing the user to diverse clusters of words associated with the concept. Users can dive into these clusters to find related, concrete objects that symbolize the concept. We evaluate SymbolFinder with two studies: a comparative user study, demonstrating that SymbolFinder helps novices find more unique symbols for abstract concepts with significantly less effort than a popular image database and a case study demonstrating how SymbolFinder helped design students create visual metaphors for three cover illustrations of news articles.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {385–399},
numpages = {15},
keywords = {design, brainstorming, symbols, interactive tool},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474758,
author = {Zhu, Junyi and Snowden, Jackson C and Verdejo, Joshua and Chen, Emily and Zhang, Paul and Ghaednia, Hamid and Schwab, Joseph H and Mueller, Stefanie},
title = {EIT-Kit: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474758},
doi = {10.1145/3472749.3474758},
abstract = {In this paper, we propose EIT-kit, an electrical impedance tomography toolkit for designing and fabricating health and motion sensing devices. EIT-kit contains (1)&nbsp;an extension to a 3D editor for personalizing the form factor of electrode arrays and electrode distribution, (2)&nbsp;a customized EIT sensing motherboard for performing the measurements, (3)&nbsp;a microcontroller library that automates signal calibration and facilitates data collection, and (4)&nbsp;an image reconstruction library for mobile devices for interpolating and visualizing the measured data. Together, these EIT-kit components allow for applications that require 2- or 4-terminal setups, up to 64 electrodes, and single or multiple (up to four) electrode arrays&nbsp;simultaneously. We motivate the design of each component of EIT-kit with a formative study, and conduct a technical evaluation of the data fidelity of our EIT measurements. We demonstrate the design space that EIT-kit enables by showing various applications in health as well as motion sensing and control. },
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {400–413},
numpages = {14},
keywords = {electronic prototyping, electrical impedance tomography, personal fabrication., health sensing},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474759,
author = {Nith, Romain and Teng, Shan-Yuan and Li, Pengyu and Tao, Yujie and Lopes, Pedro},
title = {DextrEMS: Increasing Dexterity in Electrical Muscle Stimulation by Combining It with Brakes},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474759},
doi = {10.1145/3472749.3474759},
abstract = {Electrical muscle stimulation (EMS) is an emergent technique that miniaturizes force feedback, especially popular for untethered haptic devices, such as mobile gaming, VR, or AR. However, the actuation displayed by interactive systems based on EMS is coarse and imprecise. EMS systems mostly focus on inducing movements in large muscle groups such as legs, arms, and wrists; whereas individual finger poses, which would be required, for example, to actuate a user's fingers to fingerspell even the simplest letters in sign language, are not possible. The lack of dexterity in EMS stems from two fundamental limitations: (1) lack of independence: when a particular finger is actuated by EMS, the current runs through nearby muscles, causing unwanted actuation of adjacent fingers; and, (2) unwanted oscillations: while it is relatively easy for EMS to start moving a finger, it is very hard for EMS to stop and hold that finger at a precise angle; because, to stop a finger, virtually all EMS systems contract the opposing muscle, typically achieved via controllers (e.g., PID)—unfortunately, even with the best controller tuning, this often results in unwanted oscillations. To tackle these limitations, we propose dextrEMS, an EMS-based haptic device featuring mechanical brakes attached to each finger joint. The key idea behind dextrEMS is that while the EMS actuates the fingers, it is our mechanical brake that stops the finger in a precise position. Moreover, it is also the brakes that allow dextrEMS to select which fingers are moved by EMS, eliminating unwanted movements by preventing adjacent fingers from moving. We implemented dextrEMS as an untethered haptic device, weighing only 68g, that actuates eight finger joints independently (metacarpophalangeal and proximal interphalangeal joints for four fingers), which we demonstrate in a wide range of haptic applications, such as assisted fingerspelling, a piano tutorial, guitar tutorial, and a VR game. Finally, in our technical evaluation, we found that dextrEMS outperformed EMS alone by doubling its independence and reducing unwanted oscillations.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {414–430},
numpages = {17},
keywords = {exoskeleton, dexterity, force feedback, haptics, electrical muscle stimulation},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3472749.3474760,
author = {Chen, Yu-Wen and Lin, Wei-Ju and Chen, Yi and Cheng, Lung-Pan},
title = {PneuSeries: 3D Shape Forming with Modularized Serial-Connected Inflatables},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474760},
doi = {10.1145/3472749.3474760},
abstract = { We present PneuSeries, a series of modularized inflatables where their inflation and deflation are propagated in-between stage by stage to form various shapes. The key component of PneuSeries is the bidirectional check valve that passively regulates the air flowing in/out from/to adjacent inflatables, allowing each of the inflatables to be inflated/deflated one by one through serial propagation. The form of the inflatable series thus is programmed by the sequential operations of a pump that push/pull the air in/out. In this paper, we explored the design of PneuSeries and implemented working prototypes as a proof of concept. In particular, we built PneuSeries with (1) modularized cubical, cuboidal, tetrahedral, prismatic, and custom inflatables to examine their shape forming, (2) fast assembly connectors to allow quick reconfiguration of the series, and (3) folding mechanism to reduce irregularity of the shrunken inflatables. We also evaluated the inflating and deflating time and the flow rate of the valve for simulating the inflating and deflating process and display the steps and time required to transform in our software. Finally, we demonstrate example objects that show the capability of PneuSeries and its potential applications.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {431–440},
numpages = {10},
keywords = {multi-inflatable systems, Shape-changing interface, pneumatic devices},
location = {Virtual Event, USA},
series = {UIST '21}
}

